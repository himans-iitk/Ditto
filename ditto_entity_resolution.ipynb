{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ditto for Entity Resolution (FAIR-DA4ER)\n",
        "\n",
        "This notebook implements the **Ditto** method for **Entity Resolution (ER)** as described in the [FAIR-DA4ER](https://github.com/MarcoNapoleone/FAIR-DA4ER) repository.\n",
        "\n",
        "## Overview (from README)\n",
        "\n",
        "- **FAIR-DA4ER** provides code for training **Ditto** models for Entity Resolution, with optional **FAIR-DA4ER** data augmentation.\n",
        "- **Ditto** (Li et al.) casts Entity Matching as a **sequence-pair classification** problem using pre-trained LMs (e.g. BERT, DistilBERT).\n",
        "- **Data format**: Each example is a line: `record1 \\t record2 \\t label`, where each record is serialized as `COL attr_name VAL attr_value COL ...` and label is `0` (no match) or `1` (match).\n",
        "- **Task config**: Datasets are defined in `configs.json` with `trainset`, `validset`, `testset` paths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Clone FAIR-DA4ER repo and install dependencies\n",
        "\n",
        "All configs, data paths, and code come from the [FAIR-DA4ER](https://github.com/MarcoNapoleone/FAIR-DA4ER) repository. Run the cell below to clone the repo and install from its `requirements.txt`. Subsequent cells run from the repo root."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/himanshumishra/Documents/IJF/Ditto/FAIR-DA4ER\n"
          ]
        }
      ],
      "source": [
        "# Clone FAIR-DA4ER repo (skip if already cloned)\n",
        "import os\n",
        "REPO_DIR = \"FAIR-DA4ER\"\n",
        "if not os.path.isdir(REPO_DIR):\n",
        "    !git clone https://github.com/MarcoNapoleone/FAIR-DA4ER.git {REPO_DIR}\n",
        "%cd {REPO_DIR}\n",
        "\n",
        "# Install dependencies from repo's requirements.txt\n",
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/himanshumishra/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/himanshumishra/Documents/IJF/Ditto/FAIR-DA4ER/ditto\n"
          ]
        }
      ],
      "source": [
        "cd Ditto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load task configuration (configs.json from repo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task: IJF_sentence-bert\n",
            "DITTO_DIR: /Users/himanshumishra/Documents/IJF/Ditto/FAIR-DA4ER/ditto\n",
            "Train: /Users/himanshumishra/Documents/IJF/Ditto/FAIR-DA4ER/ditto/data/ijf/sentence-bert/train.txt, Valid: /Users/himanshumishra/Documents/IJF/Ditto/FAIR-DA4ER/ditto/data/ijf/sentence-bert/valid.txt, Test: /Users/himanshumishra/Documents/IJF/Ditto/FAIR-DA4ER/ditto/data/ijf/sentence-bert/test.txt\n",
            "Train exists: True, Valid: True, Test: True\n"
          ]
        }
      ],
      "source": [
        "# Load config: use IJF_sentence-bert (data/ijf/sentence-bert/). Paths relative to ditto/.\n",
        "CONFIG_PATH = \"configs.json\"\n",
        "if not os.path.isfile(CONFIG_PATH):\n",
        "    CONFIG_PATH = os.path.join(os.path.dirname(os.path.abspath(\".\")), \"configs.json\")\n",
        "with open(CONFIG_PATH) as f:\n",
        "    configs = json.load(f)\n",
        "configs_by_name = {c[\"name\"]: c for c in configs}\n",
        "\n",
        "TASK_NAME = \"IJF_sentence-bert\"\n",
        "DITTO_DIR = os.path.dirname(os.path.abspath(CONFIG_PATH))\n",
        "\n",
        "# Resolve DITTO_DIR: if data/ijf/sentence-bert/train.txt is not here, walk up until we find it (fixes nested cd)\n",
        "IJF_TRAIN_REL = os.path.join(\"data\", \"ijf\", \"sentence-bert\", \"train.txt\")\n",
        "while DITTO_DIR != os.path.dirname(DITTO_DIR):\n",
        "    if os.path.isfile(os.path.join(DITTO_DIR, IJF_TRAIN_REL)):\n",
        "        break\n",
        "    DITTO_DIR = os.path.dirname(DITTO_DIR)\n",
        "\n",
        "if TASK_NAME in configs_by_name:\n",
        "    config = configs_by_name[TASK_NAME]\n",
        "    trainset_path = os.path.join(DITTO_DIR, config[\"trainset\"])\n",
        "    validset_path = os.path.join(DITTO_DIR, config[\"validset\"])\n",
        "    testset_path = os.path.join(DITTO_DIR, config[\"testset\"])\n",
        "else:\n",
        "    trainset_path = os.path.join(DITTO_DIR, \"data\", \"ijf\", \"sentence-bert\", \"train.txt\")\n",
        "    validset_path = os.path.join(DITTO_DIR, \"data\", \"ijf\", \"sentence-bert\", \"valid.txt\")\n",
        "    testset_path = os.path.join(DITTO_DIR, \"data\", \"ijf\", \"sentence-bert\", \"test.txt\")\n",
        "    print(f\"(IJF_sentence-bert not in configs.json; using paths under {DITTO_DIR})\")\n",
        "\n",
        "print(f\"Task: {TASK_NAME}\")\n",
        "print(f\"DITTO_DIR: {DITTO_DIR}\")\n",
        "print(f\"Train: {trainset_path}, Valid: {validset_path}, Test: {testset_path}\")\n",
        "print(f\"Train exists: {os.path.isfile(trainset_path)}, Valid: {os.path.isfile(validset_path)}, Test: {os.path.isfile(testset_path)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Ditto dataset (Ditto serialization format)\n",
        "\n",
        "Each line: `record1 \\t record2 \\t label`. Records use `COL` / `VAL` tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LICENSE                \u001b[34minput\u001b[m\u001b[m                  run_all_er_magellan.py\n",
            "README.md              matcher.py             run_all_mixup.py\n",
            "\u001b[34mblocking\u001b[m\u001b[m               \u001b[34mmixup\u001b[m\u001b[m                  run_all_vary_size.py\n",
            "configs.json           \u001b[31mmixup_da.sh\u001b[m\u001b[m            run_all_wdc.py\n",
            "\u001b[34mdata\u001b[m\u001b[m                   \u001b[31mmixup_da_v2.sh\u001b[m\u001b[m         \u001b[31mtrain_ditto.py\u001b[m\u001b[m\n",
            "ditto.jpg              \u001b[34moutput\u001b[m\u001b[m                 \u001b[31mtrain_ditto.sh\u001b[m\u001b[m\n",
            "\u001b[34mditto_light\u001b[m\u001b[m            \u001b[31mpreprocessing.sh\u001b[m\u001b[m\n",
            "download_stopwords.py  \u001b[34mresults_ditto\u001b[m\u001b[m\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 1998, Valid: 250, Test: 249 (random subsample)\n",
            "Match rate (train): 37.8%\n",
            "Sample: ('COL name VAL WORLD FUEL SERVICES COL name_clean VAL WORLD FUEL SERVICES COL sources VAL [\"fd\"] COL count VAL 1520 COL lobby_count VAL 0 COL amount VAL 41556465.18 COL amount_bc VAL null COL example VAL WORLD FUEL SERVICES', 'COL name VAL SERVICE FUEL SERVICE CANADA COL name_clean VAL SERVICE FUEL SERVICE CANADA COL sources VAL [\"fd\"] COL count VAL 1 COL lobby_count VAL 0 COL amount VAL 0 COL amount_bc VAL null COL example VAL service fuel service canada') -> 1\n"
          ]
        }
      ],
      "source": [
        "class DittoDataset(Dataset):\n",
        "    \"\"\"Dataset for Ditto ER: pairs of serialized records + binary label.\"\"\"\n",
        "\n",
        "    def __init__(self, path, tokenizer, max_len=256, size=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.pairs = []\n",
        "        self.labels = []\n",
        "\n",
        "        lines = open(path).readlines() if isinstance(path, str) else path\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split(\"\\t\")\n",
        "            if len(parts) != 3:\n",
        "                continue\n",
        "            s1, s2, label = parts[0], parts[1], int(parts[2])\n",
        "            self.pairs.append((s1, s2))\n",
        "            self.labels.append(label)\n",
        "\n",
        "        if size:\n",
        "            self.pairs = self.pairs[:size]\n",
        "            self.labels = self.labels[:size]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        left, right = self.pairs[idx]\n",
        "        # Compatible with old (encode_plus) and new (callable) transformers\n",
        "        tokenizer_fn = getattr(self.tokenizer, \"encode_plus\", self.tokenizer)\n",
        "        enc = tokenizer_fn(\n",
        "            left,\n",
        "            right,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        return {\n",
        "            \"input_ids\": torch.stack([b[\"input_ids\"] for b in batch]),\n",
        "            \"attention_mask\": torch.stack([b[\"attention_mask\"] for b in batch]),\n",
        "            \"labels\": torch.stack([b[\"labels\"] for b in batch]),\n",
        "        }\n",
        "\n",
        "# Model name (same as FAIR-DA4ER / megagonlabs ditto)\n",
        "LM_NAME = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(LM_NAME)\n",
        "\n",
        "MAX_LEN = 256\n",
        "\n",
        "# CPU-friendly: randomly subsample train/valid/test to fixed sizes\n",
        "N_TRAIN_IJF = 2000\n",
        "N_VALID_IJF = 250\n",
        "N_TEST_IJF = 250\n",
        "SEED_IJF = 42\n",
        "\n",
        "random.seed(SEED_IJF)\n",
        "np.random.seed(SEED_IJF)\n",
        "\n",
        "def load_and_sample(path, n_target):\n",
        "    \"\"\"Load file and randomly sample n_target lines (or all if file has fewer).\"\"\"\n",
        "    with open(path) as f:\n",
        "        lines = [line.strip() for line in f if line.strip()]\n",
        "    if len(lines) <= n_target:\n",
        "        return lines\n",
        "    idx = np.random.choice(len(lines), size=n_target, replace=False)\n",
        "    return [lines[i] for i in sorted(idx)]\n",
        "\n",
        "train_lines = load_and_sample(trainset_path, N_TRAIN_IJF)\n",
        "valid_lines = load_and_sample(validset_path, N_VALID_IJF)\n",
        "test_lines = load_and_sample(testset_path, N_TEST_IJF)\n",
        "\n",
        "train_ds = DittoDataset(train_lines, tokenizer, max_len=MAX_LEN)\n",
        "valid_ds = DittoDataset(valid_lines, tokenizer, max_len=MAX_LEN)\n",
        "test_ds = DittoDataset(test_lines, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "print(f\"Train: {len(train_ds)}, Valid: {len(valid_ds)}, Test: {len(test_ds)} (random subsample)\")\n",
        "print(f\"Match rate (train): {100 * np.mean(train_ds.labels):.1f}%\")\n",
        "print(\"Sample:\", train_ds.pairs[0], \"->\", train_ds.labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Ditto model (LM + linear head for binary classification)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DittoModel(nn.Module):\n",
        "    \"\"\"Ditto: pre-trained LM with a linear layer for binary match classification.\"\"\"\n",
        "\n",
        "    def __init__(self, lm_name=\"distilbert-base-uncased\", num_labels=2):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(lm_name)\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # [CLS] representation\n",
        "        pooled = outputs.last_hidden_state[:, 0, :]\n",
        "        logits = self.classifier(pooled)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_ds' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m LR = \u001b[32m3e-5\u001b[39m\n\u001b[32m     12\u001b[39m N_EPOCHS = \u001b[32m5\u001b[39m\n\u001b[32m     14\u001b[39m train_loader = DataLoader(\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[43mtrain_ds\u001b[49m,\n\u001b[32m     16\u001b[39m     batch_size=BATCH_SIZE,\n\u001b[32m     17\u001b[39m     shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     18\u001b[39m     collate_fn=DittoDataset.collate_fn,\n\u001b[32m     19\u001b[39m )\n\u001b[32m     20\u001b[39m valid_loader = DataLoader(\n\u001b[32m     21\u001b[39m     valid_ds,\n\u001b[32m     22\u001b[39m     batch_size=BATCH_SIZE * \u001b[32m2\u001b[39m,\n\u001b[32m     23\u001b[39m     shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     24\u001b[39m     collate_fn=DittoDataset.collate_fn,\n\u001b[32m     25\u001b[39m )\n\u001b[32m     26\u001b[39m test_loader = DataLoader(\n\u001b[32m     27\u001b[39m     test_ds,\n\u001b[32m     28\u001b[39m     batch_size=BATCH_SIZE * \u001b[32m2\u001b[39m,\n\u001b[32m     29\u001b[39m     shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     30\u001b[39m     collate_fn=DittoDataset.collate_fn,\n\u001b[32m     31\u001b[39m )\n",
            "\u001b[31mNameError\u001b[39m: name 'train_ds' is not defined"
          ]
        }
      ],
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "LR = 3e-5\n",
        "N_EPOCHS = 5\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=DittoDataset.collate_fn,\n",
        ")\n",
        "valid_loader = DataLoader(\n",
        "    valid_ds,\n",
        "    batch_size=BATCH_SIZE * 2,\n",
        "    shuffle=False,\n",
        "    collate_fn=DittoDataset.collate_fn,\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=BATCH_SIZE * 2,\n",
        "    shuffle=False,\n",
        "    collate_fn=DittoDataset.collate_fn,\n",
        ")\n",
        "\n",
        "model = DittoModel(lm_name=LM_NAME).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        "num_training_steps = len(train_loader) * N_EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, loader, threshold=None):\n",
        "    \"\"\"Evaluate model. If threshold is None, find optimal F1 threshold; else use it.\"\"\"\n",
        "    model.eval()\n",
        "    all_labels, all_probs = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"]\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            probs = logits.softmax(dim=1)[:, 1].cpu().numpy()\n",
        "            all_labels.extend(labels.numpy())\n",
        "            all_probs.extend(probs)\n",
        "    if threshold is not None:\n",
        "        preds = [1 if p > threshold else 0 for p in all_probs]\n",
        "        f1 = f1_score(all_labels, preds)\n",
        "        return f1, threshold, preds, all_labels\n",
        "    best_f1, best_th = 0.0, 0.5\n",
        "    for th in np.arange(0.0, 1.0, 0.05):\n",
        "        p = [1 if x > th else 0 for x in all_probs]\n",
        "        f1 = f1_score(all_labels, p)\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_th = f1, th\n",
        "    pred_best = [1 if x > best_th else 0 for x in all_probs]\n",
        "    return best_f1, best_th, pred_best, all_labels\n",
        "\n",
        "def train_epoch(model, loader, optimizer, scheduler, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | loss=0.5062 | dev_f1=0.9043 | test_f1=0.9297 | best_test_f1=0.9297\n",
            "Epoch 2 | loss=0.1321 | dev_f1=0.9462 | test_f1=0.9670 | best_test_f1=0.9670\n",
            "Epoch 3 | loss=0.0673 | dev_f1=0.9348 | test_f1=0.9724 | best_test_f1=0.9670\n",
            "Epoch 4 | loss=0.0290 | dev_f1=0.9362 | test_f1=0.9462 | best_test_f1=0.9670\n",
            "Epoch 5 | loss=0.0169 | dev_f1=0.9355 | test_f1=0.9565 | best_test_f1=0.9670\n"
          ]
        }
      ],
      "source": [
        "best_dev_f1 = 0.0\n",
        "best_test_f1 = 0.0\n",
        "\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    loss = train_epoch(model, train_loader, optimizer, scheduler, criterion)\n",
        "    dev_f1, dev_th, _, _ = evaluate(model, valid_loader)\n",
        "    test_f1, _, _, _ = evaluate(model, test_loader, threshold=dev_th)\n",
        "    if dev_f1 > best_dev_f1:\n",
        "        best_dev_f1 = dev_f1\n",
        "        best_test_f1 = test_f1\n",
        "\n",
        "    print(f\"Epoch {epoch} | loss={loss:.4f} | dev_f1={dev_f1:.4f} | test_f1={test_f1:.4f} | best_test_f1={best_test_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Final test report (using validation threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final test set performance (threshold from validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    No match       0.98      0.97      0.97       158\n",
            "       Match       0.95      0.97      0.96        91\n",
            "\n",
            "    accuracy                           0.97       249\n",
            "   macro avg       0.96      0.97      0.97       249\n",
            "weighted avg       0.97      0.97      0.97       249\n",
            "\n",
            "F1: 0.9565, Precision: 0.9462, Recall: 0.9670\n"
          ]
        }
      ],
      "source": [
        "dev_f1, dev_th, _, _ = evaluate(model, valid_loader)\n",
        "test_f1, _, test_preds, test_labels = evaluate(model, test_loader, threshold=dev_th)\n",
        "\n",
        "print(\"Final test set performance (threshold from validation):\")\n",
        "print(classification_report(test_labels, test_preds, target_names=[\"No match\", \"Match\"]))\n",
        "print(f\"F1: {f1_score(test_labels, test_preds):.4f}, Precision: {precision_score(test_labels, test_preds):.4f}, Recall: {recall_score(test_labels, test_preds):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'test_labels' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Convert labels/predictions to numpy arrays\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m y_true = np.array(\u001b[43mtest_labels\u001b[49m)\n\u001b[32m      6\u001b[39m y_pred = np.array(test_preds)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Indices for each case\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'test_labels' is not defined"
          ]
        }
      ],
      "source": [
        "# Inspect FP, FN, TP, TN examples on the IJF sentence-bert subset\n",
        "import numpy as np\n",
        "\n",
        "# Convert labels/predictions to numpy arrays\n",
        "y_true = np.array(test_labels)\n",
        "y_pred = np.array(test_preds)\n",
        "\n",
        "# Indices for each case\n",
        "tp_idx = np.where((y_true == 1) & (y_pred == 1))[0]\n",
        "fp_idx = np.where((y_true == 0) & (y_pred == 1))[0]\n",
        "tn_idx = np.where((y_true == 0) & (y_pred == 0))[0]\n",
        "fn_idx = np.where((y_true == 1) & (y_pred == 0))[0]\n",
        "\n",
        "print(f\"TP: {len(tp_idx)}, FP: {len(fp_idx)}, TN: {len(tn_idx)}, FN: {len(fn_idx)}\")\n",
        "\n",
        "\n",
        "def show_examples(ds, idxs, kind, k=5):\n",
        "    \"\"\"Pretty-print up to k examples for a given index set.\"\"\"\n",
        "    print(f\"\\n=== {kind} examples (showing up to {k}) ===\")\n",
        "    for idx in idxs[:k]:\n",
        "        rec1, rec2 = ds.pairs[idx]\n",
        "        label = ds.labels[idx]\n",
        "        print(f\"\\nIndex: {idx}\")\n",
        "        print(f\"True label: {label}\")\n",
        "        print(f\"Record 1: {rec1}\")\n",
        "        print(f\"Record 2: {rec2}\")\n",
        "\n",
        "# Show a few examples of each type from the test set\n",
        "# show_examples(test_ds, tp_idx, \"True Positives\")\n",
        "show_examples(test_ds, fp_idx, \"False Positives\")\n",
        "# show_examples(test_ds, tn_idx, \"True Negatives\")\n",
        "# show_examples(test_ds, fn_idx, \"False Negatives\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Predict on new pairs (optional)\n",
        "\n",
        "Serialize two records in Ditto format (`COL attr VAL value ...`) and get match probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Match probability: 0.0973 -> No match\n"
          ]
        }
      ],
      "source": [
        "def predict_pair(model, tokenizer, record1, record2, max_len=256):\n",
        "    \"\"\"Predict match probability for a pair of Ditto-serialized records.\"\"\"\n",
        "    enc = tokenizer(\n",
        "        record1,\n",
        "        record2,\n",
        "        max_length=max_len,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device))\n",
        "        prob_match = logits.softmax(dim=1)[0, 1].item()\n",
        "    return prob_match\n",
        "\n",
        "# Example\n",
        "r1 = \"COL title VAL samsung galaxy COL brand VAL samsung\"\n",
        "r2 = \"COL title VAL samsung galaxy s21 COL brand VAL samsung\"\n",
        "prob = predict_pair(model, tokenizer, r1, r2)\n",
        "print(f\"Match probability: {prob:.4f} -> {'Match' if prob >= 0.5 else 'No match'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part B: Ditto with Data Augmentation (DA_iTunes-Amazon)\n",
        "\n",
        "This section trains Ditto on **DA_iTunes-Amazon** using **in-training data augmentation (MixDA)** from the repo: the train set uses the `ditto_light` dataset with `da='del'` (span deletion), and the model mixes original and augmented representations during training. Valid/test use no augmentation. Results are reported as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task (with DA): DA_iTunes-Amazon\n",
            "Train: data/fair/iTunes-Amazon/train.txt, Valid: data/fair/iTunes-Amazon/valid.txt, Test: data/fair/iTunes-Amazon/test.txt\n"
          ]
        }
      ],
      "source": [
        "# Ensure we're in the repo's ditto directory (paths are relative to it)\n",
        "import os\n",
        "try:\n",
        "    _ditto_dir = os.path.join(REPO_DIR, \"ditto\")\n",
        "except NameError:\n",
        "    _ditto_dir = os.path.join(\"FAIR-DA4ER\", \"ditto\")\n",
        "if os.path.isdir(_ditto_dir) and not os.path.isfile(\"configs.json\"):\n",
        "    os.chdir(_ditto_dir)\n",
        "# Load config for DA_iTunes-Amazon (configs[1])\n",
        "with open(\"configs.json\") as f:\n",
        "    configs_da = json.load(f)\n",
        "TASK_NAME_DA = configs_da[1][\"name\"]  # DA_iTunes-Amazon\n",
        "config_da = {c[\"name\"]: c for c in configs_da}[TASK_NAME_DA]\n",
        "trainset_path_da = config_da[\"trainset\"]\n",
        "validset_path_da = config_da[\"validset\"]\n",
        "testset_path_da = config_da[\"testset\"]\n",
        "print(f\"Task (with DA): {TASK_NAME_DA}\")\n",
        "print(f\"Train: {trainset_path_da}, Valid: {validset_path_da}, Test: {testset_path_da}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 321, Valid: 109, Test: 109 (with DA op=del on train)\n"
          ]
        }
      ],
      "source": [
        "# Use ditto_light dataset with in-training augmentation (da='del') for train; no DA for valid/test\n",
        "import sys\n",
        "_ditto_abs = os.path.abspath(_ditto_dir)\n",
        "if _ditto_abs not in sys.path:\n",
        "    sys.path.insert(0, _ditto_abs)\n",
        "from ditto_light.dataset import DittoDataset as DittoDatasetDA\n",
        "from ditto_light.ditto import DittoModel as DittoModelDA\n",
        "\n",
        "MAX_LEN_DA = 256\n",
        "DA_OP = \"del\"  # span deletion (or 'all' for RandAugment)\n",
        "train_ds_da = DittoDatasetDA(trainset_path_da, lm=\"distilbert\", max_len=MAX_LEN_DA, da=DA_OP)\n",
        "valid_ds_da = DittoDatasetDA(validset_path_da, lm=\"distilbert\", max_len=MAX_LEN_DA, da=None)\n",
        "test_ds_da = DittoDatasetDA(testset_path_da, lm=\"distilbert\", max_len=MAX_LEN_DA, da=None)\n",
        "\n",
        "BATCH_SIZE_DA = 8\n",
        "train_loader_da = DataLoader(train_ds_da, batch_size=BATCH_SIZE_DA, shuffle=True, collate_fn=DittoDatasetDA.pad)\n",
        "valid_loader_da = DataLoader(valid_ds_da, batch_size=BATCH_SIZE_DA * 2, shuffle=False, collate_fn=DittoDatasetDA.pad)\n",
        "test_loader_da = DataLoader(test_ds_da, batch_size=BATCH_SIZE_DA * 2, shuffle=False, collate_fn=DittoDatasetDA.pad)\n",
        "print(f\"Train: {len(train_ds_da)}, Valid: {len(valid_ds_da)}, Test: {len(test_ds_da)} (with DA op={DA_OP} on train)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model with MixDA (mixes original + augmented [CLS] during training)\n",
        "set_seed(42)\n",
        "model_da = DittoModelDA(device=device, lm=\"distilbert\", alpha_aug=0.8).to(device)\n",
        "optimizer_da = AdamW(model_da.parameters(), lr=LR)\n",
        "num_steps_da = len(train_loader_da) * N_EPOCHS\n",
        "scheduler_da = get_linear_schedule_with_warmup(optimizer_da, num_warmup_steps=0, num_training_steps=num_steps_da)\n",
        "criterion_da = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch_da(model, loader, optimizer, scheduler, criterion):\n",
        "    \"\"\"Train one epoch; loader returns (x1, x2, y) when DA is used, (x, y) otherwise.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in loader:\n",
        "        optimizer.zero_grad()\n",
        "        if len(batch) == 3:\n",
        "            x1, x2, y = batch\n",
        "            logits = model(x1, x2)\n",
        "        else:\n",
        "            x, y = batch\n",
        "            logits = model(x)\n",
        "        loss = criterion(logits, y.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate_da(model, loader, threshold=None):\n",
        "    \"\"\"Evaluate; valid/test loaders return (x, y) only.\"\"\"\n",
        "    model.eval()\n",
        "    all_labels, all_probs = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            if len(batch) == 3:\n",
        "                x, _, y = batch\n",
        "            else:\n",
        "                x, y = batch\n",
        "            logits = model(x)\n",
        "            probs = logits.softmax(dim=1)[:, 1].cpu().numpy()\n",
        "            all_labels.extend(y.numpy())\n",
        "            all_probs.extend(probs)\n",
        "    if threshold is not None:\n",
        "        preds = [1 if p > threshold else 0 for p in all_probs]\n",
        "        return f1_score(all_labels, preds), threshold, preds, all_labels\n",
        "    best_f1, best_th = 0.0, 0.5\n",
        "    for th in np.arange(0.0, 1.0, 0.05):\n",
        "        p = [1 if x > th else 0 for x in all_probs]\n",
        "        f1 = f1_score(all_labels, p)\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_th = f1, th\n",
        "    pred_best = [1 if x > best_th else 0 for x in all_probs]\n",
        "    return best_f1, best_th, pred_best, all_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## IJF Blocking with Sentence-BERT + Jaro-Winkler Filtering\n",
        "\n",
        "Load `pro_supplier_with_clean_and_canonical_trimmed.csv`, apply sentence-BERT blocking (top-20 per record), filter by Jaro-Winkler similarity (>= 0.5), then create train/test/valid splits and train Ditto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1388738 records from CSV\n",
            "Sample record: COL clean_supplier_name VAL SIMZER DESIGN COL address VAL  COL city VAL  COL prov VAL  COL postal VAL  COL country VAL CA...\n",
            "Canonical ints range: 1 to 157859\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load CSV and serialize records to Ditto format\n",
        "import csv\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# Path to the CSV\n",
        "CSV_PATH_BLOCKING = \"data/ijf/blocking_sentence_bert/pro_supplier_with_clean_and_canonical_trimmed.csv\"\n",
        "\n",
        "# Ensure we're in ditto directory - try multiple paths\n",
        "if not os.path.isfile(CSV_PATH_BLOCKING):\n",
        "    CSV_PATH_BLOCKING = os.path.join(DITTO_DIR, CSV_PATH_BLOCKING)\n",
        "if not os.path.isfile(CSV_PATH_BLOCKING):\n",
        "    # Try relative to current working directory\n",
        "    CSV_PATH_BLOCKING = os.path.join(\"FAIR-DA4ER\", \"ditto\", CSV_PATH_BLOCKING)\n",
        "\n",
        "# Load CSV and serialize to Ditto format\n",
        "records_blocking = []\n",
        "canonical_ints = []\n",
        "with open(CSV_PATH_BLOCKING, encoding=\"utf-8\", newline=\"\") as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    columns = [c for c in reader.fieldnames if c != \"canonical_int\"]  # exclude canonical_int from record text\n",
        "    for i, row in enumerate(reader):\n",
        "        rec_str = \" \".join([f\"COL {col} VAL {row.get(col, '').strip()}\" for col in columns])\n",
        "        records_blocking.append((str(i), rec_str))\n",
        "        canonical_ints.append(int(row.get(\"canonical_int\", 0)))\n",
        "\n",
        "print(f\"Loaded {len(records_blocking)} records from CSV\")\n",
        "print(f\"Sample record: {records_blocking[0][1][:200]}...\")\n",
        "print(f\"Canonical ints range: {min(canonical_ints)} to {max(canonical_ints)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying sentence-BERT blocking (top-20 per record)...\n",
            "This may take a while for large datasets...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1558.29it/s, Materializing param=pooler.dense.weight]                             \n",
            "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mApplying sentence-BERT blocking (top-20 per record)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThis may take a while for large datasets...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m pairs_bert = \u001b[43mblocking_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrecords_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrecords_right\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# self-join\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# will use default \"all-MiniLM-L6-v2\"\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# top-20 most similar per record\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAfter sentence-BERT blocking: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pairs_bert)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m candidate pairs\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/IJF/Ditto/FAIR-DA4ER/ditto/blocking/blocking.py:358\u001b[39m, in \u001b[36mblocking_embedding\u001b[39m\u001b[34m(records_left, records_right, model, k, threshold, batch_size)\u001b[39m\n\u001b[32m    355\u001b[39m lines_left = [rec \u001b[38;5;28;01mfor\u001b[39;00m _, rec \u001b[38;5;129;01min\u001b[39;00m left]\n\u001b[32m    356\u001b[39m lines_right = [rec \u001b[38;5;28;01mfor\u001b[39;00m _, rec \u001b[38;5;129;01min\u001b[39;00m right]\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m vec_left = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlines_left\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    359\u001b[39m vec_left = np.array([v / (np.linalg.norm(v) + \u001b[32m1e-9\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m vec_left])\n\u001b[32m    360\u001b[39m vec_right = model.encode(lines_right)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/utils/_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:1130\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m   1128\u001b[39m             \u001b[38;5;66;03m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[39;00m\n\u001b[32m   1129\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m convert_to_numpy:\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m                 embeddings = \u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1132\u001b[39m         all_embeddings.extend(embeddings)\n\u001b[32m   1134\u001b[39m all_embeddings = [all_embeddings[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np.argsort(length_sorted_idx)]\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Step 2: Apply sentence-BERT blocking (top-20 per record)\n",
        "import sys\n",
        "\n",
        "# Add blocking directory to path\n",
        "blocking_dir = os.path.join(DITTO_DIR, \"blocking\")\n",
        "if blocking_dir not in sys.path:\n",
        "    sys.path.insert(0, blocking_dir)\n",
        "\n",
        "from blocking import blocking_embedding\n",
        "\n",
        "# Import sentence transformer if needed\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except ImportError:\n",
        "    print(\"Installing sentence-transformers...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sentence-transformers\"])\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "\n",
        "print(\"Applying sentence-BERT blocking (top-20 per record)...\")\n",
        "print(\"This may take a while for large datasets...\")\n",
        "pairs_bert = blocking_embedding(\n",
        "    records_blocking,\n",
        "    records_right=None,  # self-join\n",
        "    model=None,  # will use default \"all-MiniLM-L6-v2\"\n",
        "    k=20,  # top-20 most similar per record\n",
        "    threshold=None,\n",
        "    batch_size=512,\n",
        ")\n",
        "\n",
        "print(f\"After sentence-BERT blocking: {len(pairs_bert)} candidate pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Filter by Jaro-Winkler similarity (>= 0.5) on clean_supplier_name\n",
        "try:\n",
        "    from rapidfuzz.distance import JaroWinkler\n",
        "    jaro_winkler_fn = lambda s1, s2: JaroWinkler.normalized_similarity(s1, s2)\n",
        "except ImportError:\n",
        "    try:\n",
        "        from jellyfish import jaro_winkler_similarity as jaro_winkler_fn\n",
        "    except ImportError:\n",
        "        print(\"Installing rapidfuzz for Jaro-Winkler...\")\n",
        "        import subprocess\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"rapidfuzz\"])\n",
        "        from rapidfuzz.distance import JaroWinkler\n",
        "        jaro_winkler_fn = lambda s1, s2: JaroWinkler.normalized_similarity(s1, s2)\n",
        "\n",
        "# Extract clean_supplier_name from Ditto records\n",
        "def extract_clean_supplier_name(rec_str):\n",
        "    \"\"\"Extract clean_supplier_name value from Ditto record.\"\"\"\n",
        "    # Pattern: COL clean_supplier_name VAL <value> followed by COL or end of string\n",
        "    pattern = r\"COL\\s+clean_supplier_name\\s+VAL\\s+(\\S+(?:\\s+\\S+)*?)(?=\\s+COL\\s+|\\s*$)\"\n",
        "    match = re.search(pattern, rec_str, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    return \"\"\n",
        "\n",
        "# Filter pairs by Jaro-Winkler similarity >= 0.5\n",
        "JARO_WINKLER_THRESHOLD = 0.5\n",
        "pairs_filtered = []\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"Filtering pairs by Jaro-Winkler similarity...\")\n",
        "for i, j in tqdm(pairs_bert, desc=\"Jaro-Winkler filtering\"):\n",
        "    rec1_str = records_blocking[i][1]\n",
        "    rec2_str = records_blocking[j][1]\n",
        "    name1 = extract_clean_supplier_name(rec1_str)\n",
        "    name2 = extract_clean_supplier_name(rec2_str)\n",
        "    jw_sim = jaro_winkler_fn(name1, name2)\n",
        "    if jw_sim >= JARO_WINKLER_THRESHOLD:\n",
        "        pairs_filtered.append((i, j))\n",
        "\n",
        "print(f\"After Jaro-Winkler filtering (>= {JARO_WINKLER_THRESHOLD}): {len(pairs_filtered)} pairs\")\n",
        "print(f\"Filtered out: {len(pairs_bert) - len(pairs_filtered)} pairs ({100*(len(pairs_bert)-len(pairs_filtered))/len(pairs_bert) if len(pairs_bert) > 0 else 0:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Label pairs (same canonical_int = 1) and split into train/test/valid\n",
        "labeled_pairs = []\n",
        "for i, j in pairs_filtered:\n",
        "    label = 1 if canonical_ints[i] == canonical_ints[j] and canonical_ints[i] != 0 else 0\n",
        "    labeled_pairs.append((i, j, label))\n",
        "\n",
        "labeled_pairs = np.array(labeled_pairs, dtype=object)\n",
        "labels = np.array([x[2] for x in labeled_pairs])\n",
        "n_pos = int(np.sum(labels == 1))\n",
        "n_neg = int(np.sum(labels == 0))\n",
        "\n",
        "print(f\"Labeled pairs: {len(labeled_pairs)}\")\n",
        "print(f\"Matches (label=1): {n_pos} ({100*n_pos/len(labeled_pairs):.1f}%)\")\n",
        "print(f\"Non-matches (label=0): {n_neg} ({100*n_neg/len(labeled_pairs):.1f}%)\")\n",
        "\n",
        "# Stratified split: 80% train, 10% valid, 10% test\n",
        "SEED_BLOCKING = 42\n",
        "np.random.seed(SEED_BLOCKING)\n",
        "pos_idx = np.where(labels == 1)[0]\n",
        "neg_idx = np.where(labels == 0)[0]\n",
        "np.random.shuffle(pos_idx)\n",
        "np.random.shuffle(neg_idx)\n",
        "\n",
        "def split_indices(idxs, train_ratio=0.8, valid_ratio=0.1):\n",
        "    n = len(idxs)\n",
        "    if n == 0:\n",
        "        return [], [], []\n",
        "    t = max(1, int(n * train_ratio))\n",
        "    v = max(0, int(n * valid_ratio))\n",
        "    te = n - t - v\n",
        "    return idxs[:t], idxs[t:t+v], idxs[t+v:]\n",
        "\n",
        "pos_t, pos_v, pos_te = split_indices(pos_idx)\n",
        "neg_t, neg_v, neg_te = split_indices(neg_idx)\n",
        "\n",
        "train_idx = np.concatenate([pos_t, neg_t])\n",
        "valid_idx = np.concatenate([pos_v, neg_v])\n",
        "test_idx = np.concatenate([pos_te, neg_te])\n",
        "np.random.shuffle(train_idx)\n",
        "np.random.shuffle(valid_idx)\n",
        "np.random.shuffle(test_idx)\n",
        "\n",
        "print(f\"\\nSplit sizes:\")\n",
        "print(f\"  Train: {len(train_idx)}\")\n",
        "print(f\"  Valid: {len(valid_idx)}\")\n",
        "print(f\"  Test: {len(test_idx)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Write train.txt, test.txt, valid.txt\n",
        "OUTPUT_DIR_BLOCKING = os.path.join(DITTO_DIR, \"data/ijf/blocking_sentence_bert\")\n",
        "os.makedirs(OUTPUT_DIR_BLOCKING, exist_ok=True)\n",
        "\n",
        "def write_split(split_name, indices):\n",
        "    path = os.path.join(OUTPUT_DIR_BLOCKING, f\"{split_name}.txt\")\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for idx in indices:\n",
        "            i, j, label = labeled_pairs[idx]\n",
        "            rec1 = records_blocking[i][1]\n",
        "            rec2 = records_blocking[j][1]\n",
        "            f.write(f\"{rec1}\\t{rec2}\\t{label}\\n\")\n",
        "    return path\n",
        "\n",
        "train_path = write_split(\"train\", train_idx)\n",
        "valid_path = write_split(\"valid\", valid_idx)\n",
        "test_path = write_split(\"test\", test_idx)\n",
        "\n",
        "print(f\"Wrote train.txt, valid.txt, test.txt to {OUTPUT_DIR_BLOCKING}\")\n",
        "\n",
        "# Report statistics\n",
        "train_labels = np.array([labeled_pairs[i][2] for i in train_idx])\n",
        "valid_labels = np.array([labeled_pairs[i][2] for i in valid_idx])\n",
        "test_labels_blocking = np.array([labeled_pairs[i][2] for i in test_idx])\n",
        "\n",
        "print(f\"\\n=== Dataset Statistics ===\")\n",
        "print(f\"Train: {len(train_idx)} pairs | Matches: {int(train_labels.sum())} ({100*train_labels.mean():.1f}%) | Non-matches: {len(train_idx)-int(train_labels.sum())} ({100*(1-train_labels.mean()):.1f}%)\")\n",
        "print(f\"Valid: {len(valid_idx)} pairs | Matches: {int(valid_labels.sum())} ({100*valid_labels.mean():.1f}%) | Non-matches: {len(valid_idx)-int(valid_labels.sum())} ({100*(1-valid_labels.mean()):.1f}%)\")\n",
        "print(f\"Test:  {len(test_idx)} pairs | Matches: {int(test_labels_blocking.sum())} ({100*test_labels_blocking.mean():.1f}%) | Non-matches: {len(test_idx)-int(test_labels_blocking.sum())} ({100*(1-test_labels_blocking.mean()):.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Training loop (blocking + Jaro-Winkler dataset)\n",
        "set_seed(SEED_BLOCKING)\n",
        "best_dev_f1_blocking = 0.0\n",
        "best_test_f1_blocking = 0.0\n",
        "\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    loss = train_epoch(\n",
        "        model_blocking,\n",
        "        train_loader_blocking,\n",
        "        optimizer_blocking,\n",
        "        scheduler_blocking,\n",
        "        criterion_blocking,\n",
        "    )\n",
        "    dev_f1, dev_th, _, _ = evaluate(model_blocking, valid_loader_blocking)\n",
        "    test_f1, _, _, _ = evaluate(model_blocking, test_loader_blocking, threshold=dev_th)\n",
        "\n",
        "    if dev_f1 > best_dev_f1_blocking:\n",
        "        best_dev_f1_blocking = dev_f1\n",
        "        best_test_f1_blocking = test_f1\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch} | loss={loss:.4f} \"\n",
        "        f\"| dev_f1={dev_f1:.4f} | test_f1={test_f1:.4f} \"\n",
        "        f\"| best_test_f1={best_test_f1_blocking:.4f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Load datasets and train Ditto\n",
        "train_ds_blocking = DittoDataset(train_path, tokenizer, max_len=MAX_LEN)\n",
        "valid_ds_blocking = DittoDataset(valid_path, tokenizer, max_len=MAX_LEN)\n",
        "test_ds_blocking = DittoDataset(test_path, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "train_loader_blocking = DataLoader(train_ds_blocking, batch_size=BATCH_SIZE, shuffle=True, collate_fn=DittoDataset.collate_fn)\n",
        "valid_loader_blocking = DataLoader(valid_ds_blocking, batch_size=BATCH_SIZE * 2, shuffle=False, collate_fn=DittoDataset.collate_fn)\n",
        "test_loader_blocking = DataLoader(test_ds_blocking, batch_size=BATCH_SIZE * 2, shuffle=False, collate_fn=DittoDataset.collate_fn)\n",
        "\n",
        "# Create new model for this run\n",
        "model_blocking = DittoModel(lm_name=LM_NAME).to(device)\n",
        "optimizer_blocking = AdamW(model_blocking.parameters(), lr=LR)\n",
        "num_steps_blocking = len(train_loader_blocking) * N_EPOCHS\n",
        "scheduler_blocking = get_linear_schedule_with_warmup(optimizer_blocking, num_warmup_steps=0, num_training_steps=num_steps_blocking)\n",
        "criterion_blocking = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"Train dataset: {len(train_ds_blocking)} pairs\")\n",
        "print(f\"Valid dataset: {len(valid_ds_blocking)} pairs\")\n",
        "print(f\"Test dataset: {len(test_ds_blocking)} pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 8: Final evaluation on test set with detailed metrics\n",
        "dev_f1_blocking, dev_th_blocking, _, _ = evaluate(model_blocking, valid_loader_blocking)\n",
        "test_f1_blocking, _, test_preds_blocking, test_labels_blocking_final = evaluate(model_blocking, test_loader_blocking, threshold=dev_th_blocking)\n",
        "\n",
        "# Compute TP, FP, TN, FN\n",
        "y_true = np.array(test_labels_blocking_final)\n",
        "y_pred = np.array(test_preds_blocking)\n",
        "\n",
        "tp = int(np.sum((y_true == 1) & (y_pred == 1)))\n",
        "fp = int(np.sum((y_true == 0) & (y_pred == 1)))\n",
        "tn = int(np.sum((y_true == 0) & (y_pred == 0)))\n",
        "fn = int(np.sum((y_true == 1) & (y_pred == 0)))\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = (tp + tn) / len(y_true) if len(y_true) > 0 else 0.0\n",
        "precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "print(\"=== Final Test Set Performance (IJF Blocking + Jaro-Winkler) ===\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"No match\", \"Match\"]))\n",
        "print(f\"\\n=== Detailed Metrics ===\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1:        {f1:.4f}\")\n",
        "print(f\"\\n=== Confusion Matrix ===\")\n",
        "print(f\"TP (True Positives):  {tp}\")\n",
        "print(f\"FP (False Positives): {fp}\")\n",
        "print(f\"TN (True Negatives):  {tn}\")\n",
        "print(f\"FN (False Negatives): {fn}\")\n",
        "print(f\"\\nTotal test pairs: {len(y_true)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 9: Show examples of TP, FP, TN, FN\n",
        "tp_idx_blocking = np.where((y_true == 1) & (y_pred == 1))[0]\n",
        "fp_idx_blocking = np.where((y_true == 0) & (y_pred == 1))[0]\n",
        "tn_idx_blocking = np.where((y_true == 0) & (y_pred == 0))[0]\n",
        "fn_idx_blocking = np.where((y_true == 1) & (y_pred == 0))[0]\n",
        "\n",
        "print(f\"\\n=== Example Analysis ===\")\n",
        "show_examples(test_ds_blocking, tp_idx_blocking, \"True Positives (Blocking)\", k=3)\n",
        "show_examples(test_ds_blocking, fp_idx_blocking, \"False Positives (Blocking)\", k=3)\n",
        "show_examples(test_ds_blocking, tn_idx_blocking, \"True Negatives (Blocking)\", k=3)\n",
        "show_examples(test_ds_blocking, fn_idx_blocking, \"False Negatives (Blocking)\", k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect FP, FN, TP, TN examples on the IJF sentence-bert subset\n",
        "import numpy as np\n",
        "\n",
        "# Convert labels/predictions to numpy arrays\n",
        "y_true = np.array(test_labels)\n",
        "y_pred = np.array(test_preds)\n",
        "\n",
        "# Indices for each case\n",
        "tp_idx = np.where((y_true == 1) & (y_pred == 1))[0]\n",
        "fp_idx = np.where((y_true == 0) & (y_pred == 1))[0]\n",
        "tn_idx = np.where((y_true == 0) & (y_pred == 0))[0]\n",
        "fn_idx = np.where((y_true == 1) & (y_pred == 0))[0]\n",
        "\n",
        "print(f\"TP: {len(tp_idx)}, FP: {len(fp_idx)}, TN: {len(tn_idx)}, FN: {len(fn_idx)}\")\n",
        "\n",
        "\n",
        "def show_examples(ds, idxs, kind, k=5):\n",
        "    \"\"\"Pretty-print up to k examples for a given index set.\"\"\"\n",
        "    print(f\"\\n=== {kind} examples (showing up to {k}) ===\")\n",
        "    for idx in idxs[:k]:\n",
        "        rec1, rec2 = ds.pairs[idx]\n",
        "        label = ds.labels[idx]\n",
        "        print(f\"\\nIndex: {idx}\")\n",
        "        print(f\"True label: {label}\")\n",
        "        print(f\"Record 1: {rec1}\")\n",
        "        print(f\"Record 2: {rec2}\")\n",
        "\n",
        "# Show a few examples of each type from the test set\n",
        "show_examples(test_ds, tp_idx, \"True Positives\")\n",
        "show_examples(test_ds, fp_idx, \"False Positives\")\n",
        "show_examples(test_ds, tn_idx, \"True Negatives\")\n",
        "show_examples(test_ds, fn_idx, \"False Negatives\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | loss=0.5796 | dev_f1=0.5806 | test_f1=0.5926 | best_test_f1=0.5926\n",
            "Epoch 2 | loss=0.4715 | dev_f1=0.6316 | test_f1=0.6667 | best_test_f1=0.6667\n",
            "Epoch 3 | loss=0.2959 | dev_f1=0.8475 | test_f1=0.8621 | best_test_f1=0.8621\n",
            "Epoch 4 | loss=0.1591 | dev_f1=0.8966 | test_f1=0.9123 | best_test_f1=0.9123\n",
            "Epoch 5 | loss=0.1178 | dev_f1=0.8889 | test_f1=0.8889 | best_test_f1=0.9123\n",
            "Epoch 6 | loss=0.0505 | dev_f1=0.8889 | test_f1=0.9091 | best_test_f1=0.9123\n",
            "Epoch 7 | loss=0.1673 | dev_f1=0.8814 | test_f1=0.8966 | best_test_f1=0.9123\n",
            "Epoch 8 | loss=0.0305 | dev_f1=0.8772 | test_f1=0.9310 | best_test_f1=0.9123\n",
            "Epoch 9 | loss=0.0241 | dev_f1=0.8772 | test_f1=0.9310 | best_test_f1=0.9123\n",
            "Epoch 10 | loss=0.0134 | dev_f1=0.8772 | test_f1=0.9123 | best_test_f1=0.9123\n"
          ]
        }
      ],
      "source": [
        "# Training loop (Ditto with DA)\n",
        "best_dev_f1_da = 0.0\n",
        "best_test_f1_da = 0.0\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    loss = train_epoch_da(model_da, train_loader_da, optimizer_da, scheduler_da, criterion_da)\n",
        "    dev_f1, dev_th, _, _ = evaluate_da(model_da, valid_loader_da)\n",
        "    test_f1, _, _, _ = evaluate_da(model_da, test_loader_da, threshold=dev_th)\n",
        "    if dev_f1 > best_dev_f1_da:\n",
        "        best_dev_f1_da = dev_f1\n",
        "        best_test_f1_da = test_f1\n",
        "    print(f\"Epoch {epoch} | loss={loss:.4f} | dev_f1={dev_f1:.4f} | test_f1={test_f1:.4f} | best_test_f1={best_test_f1_da:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'evaluate_da' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Final test report (Ditto with DA) — same format as before\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dev_f1_da, dev_th_da, _, _ = \u001b[43mevaluate_da\u001b[49m(model_da, valid_loader_da)\n\u001b[32m      3\u001b[39m test_f1_da, _, test_preds_da, test_labels_da = evaluate_da(model_da, test_loader_da, threshold=dev_th_da)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPart B — \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTASK_NAME_DA\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (with data augmentation):\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'evaluate_da' is not defined"
          ]
        }
      ],
      "source": [
        "# Final test report (Ditto with DA) — same format as before\n",
        "dev_f1_da, dev_th_da, _, _ = evaluate_da(model_da, valid_loader_da)\n",
        "test_f1_da, _, test_preds_da, test_labels_da = evaluate_da(model_da, test_loader_da, threshold=dev_th_da)\n",
        "print(f\"Part B — {TASK_NAME_DA} (with data augmentation):\")\n",
        "print(\"Final test set performance (threshold from validation):\")\n",
        "print(classification_report(test_labels_da, test_preds_da, target_names=[\"No match\", \"Match\"]))\n",
        "print(f\"F1: {f1_score(test_labels_da, test_preds_da):.4f}, Precision: {precision_score(test_labels_da, test_preds_da):.4f}, Recall: {recall_score(test_labels_da, test_preds_da):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
