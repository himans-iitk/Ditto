{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ditto for Entity Resolution (FAIR-DA4ER)\n",
        "\n",
        "This notebook implements the **Ditto** method for **Entity Resolution (ER)** as described in the [FAIR-DA4ER](https://github.com/MarcoNapoleone/FAIR-DA4ER) repository.\n",
        "\n",
        "## Overview (from README)\n",
        "\n",
        "- **FAIR-DA4ER** provides code for training **Ditto** models for Entity Resolution, with optional **FAIR-DA4ER** data augmentation.\n",
        "- **Ditto** (Li et al.) casts Entity Matching as a **sequence-pair classification** problem using pre-trained LMs (e.g. BERT, DistilBERT).\n",
        "- **Data format**: Each example is a line: `record1 \\t record2 \\t label`, where each record is serialized as `COL attr_name VAL attr_value COL ...` and label is `0` (no match) or `1` (match).\n",
        "- **Task config**: Datasets are defined in `configs.json` with `trainset`, `validset`, `testset` paths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Clone FAIR-DA4ER repo and install dependencies\n",
        "\n",
        "All configs, data paths, and code come from the [FAIR-DA4ER](https://github.com/MarcoNapoleone/FAIR-DA4ER) repository. Run the cell below to clone the repo and install from its `requirements.txt`. Subsequent cells run from the repo root."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'FAIR-DA4ER'...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 469, done.\u001b[K\n",
            "remote: Counting objects: 100% (469/469), done.\u001b[K\n",
            "remote: Compressing objects: 100% (217/217), done.\u001b[K\n",
            "remote: Total 469 (delta 232), reused 467 (delta 230), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (469/469), 35.38 MiB | 14.50 MiB/s, done.\n",
            "Resolving deltas: 100% (232/232), done.\n",
            "Updating files: 100% (160/160), done.\n",
            "/workspace/Ditto/FAIR-DA4ER/FAIR-DA4ER\n"
          ]
        }
      ],
      "source": [
        "# Clone FAIR-DA4ER repo (skip if already cloned)\n",
        "import os\n",
        "REPO_DIR = \"FAIR-DA4ER\"\n",
        "if not os.path.isdir(REPO_DIR):\n",
        "    !git clone https://github.com/MarcoNapoleone/FAIR-DA4ER.git {REPO_DIR}\n",
        "%cd {REPO_DIR}\n",
        "\n",
        "# Install dependencies from repo's requirements.txt\n",
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.13 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies into the same Python this kernel uses\n",
        "import sys\n",
        "!{sys.executable} -m pip install -q numpy torch transformers scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.13/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace/Ditto/FAIR-DA4ER/FAIR-DA4ER/ditto\n"
          ]
        }
      ],
      "source": [
        "cd ditto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load task configuration (configs.json from repo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(IJF_sentence-bert not in configs.json; using paths under /workspace/Ditto/FAIR-DA4ER/FAIR-DA4ER/ditto)\n",
            "Task: IJF_sentence-bert\n",
            "DITTO_DIR: /workspace/Ditto/FAIR-DA4ER/FAIR-DA4ER/ditto\n",
            "Train: /workspace/Ditto/FAIR-DA4ER/FAIR-DA4ER/ditto/data/ijf/sentence-bert/train.txt, Valid: /workspace/Ditto/FAIR-DA4ER/FAIR-DA4ER/ditto/data/ijf/sentence-bert/valid.txt, Test: /workspace/Ditto/FAIR-DA4ER/FAIR-DA4ER/ditto/data/ijf/sentence-bert/test.txt\n",
            "Train exists: True, Valid: True, Test: True\n"
          ]
        }
      ],
      "source": [
        "# Load config: use IJF_sentence-bert (data/ijf/sentence-bert/). Paths relative to ditto/.\n",
        "CONFIG_PATH = \"configs.json\"\n",
        "if not os.path.isfile(CONFIG_PATH):\n",
        "    CONFIG_PATH = os.path.join(os.path.dirname(os.path.abspath(\".\")), \"configs.json\")\n",
        "with open(CONFIG_PATH) as f:\n",
        "    configs = json.load(f)\n",
        "configs_by_name = {c[\"name\"]: c for c in configs}\n",
        "\n",
        "TASK_NAME = \"IJF_sentence-bert\"\n",
        "DITTO_DIR = os.path.dirname(os.path.abspath(CONFIG_PATH))\n",
        "\n",
        "# Resolve DITTO_DIR: if data/ijf/sentence-bert/train.txt is not here, walk up until we find it (fixes nested cd)\n",
        "IJF_TRAIN_REL = os.path.join(\"data\", \"ijf\", \"sentence-bert\", \"train.txt\")\n",
        "while DITTO_DIR != os.path.dirname(DITTO_DIR):\n",
        "    if os.path.isfile(os.path.join(DITTO_DIR, IJF_TRAIN_REL)):\n",
        "        break\n",
        "    DITTO_DIR = os.path.dirname(DITTO_DIR)\n",
        "\n",
        "if TASK_NAME in configs_by_name:\n",
        "    config = configs_by_name[TASK_NAME]\n",
        "    trainset_path = os.path.join(DITTO_DIR, config[\"trainset\"])\n",
        "    validset_path = os.path.join(DITTO_DIR, config[\"validset\"])\n",
        "    testset_path = os.path.join(DITTO_DIR, config[\"testset\"])\n",
        "else:\n",
        "    trainset_path = os.path.join(DITTO_DIR, \"data\", \"ijf\", \"sentence-bert\", \"train.txt\")\n",
        "    validset_path = os.path.join(DITTO_DIR, \"data\", \"ijf\", \"sentence-bert\", \"valid.txt\")\n",
        "    testset_path = os.path.join(DITTO_DIR, \"data\", \"ijf\", \"sentence-bert\", \"test.txt\")\n",
        "    print(f\"(IJF_sentence-bert not in configs.json; using paths under {DITTO_DIR})\")\n",
        "\n",
        "print(f\"Task: {TASK_NAME}\")\n",
        "print(f\"DITTO_DIR: {DITTO_DIR}\")\n",
        "print(f\"Train: {trainset_path}, Valid: {validset_path}, Test: {testset_path}\")\n",
        "print(f\"Train exists: {os.path.isfile(trainset_path)}, Valid: {os.path.isfile(validset_path)}, Test: {os.path.isfile(testset_path)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Ditto dataset (Ditto serialization format)\n",
        "\n",
        "Each line: `record1 \\t record2 \\t label`. Records use `COL` / `VAL` tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[34;42m3-gram\u001b[0m/                  build_train_valid_test.py           \u001b[34;42msentence-bert\u001b[0m/\n",
            "\u001b[34;42mblocking_sentence_bert\u001b[0m/  pro_supplier_standardization_v.csv\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 1998, Valid: 250, Test: 249 (random subsample)\n",
            "Match rate (train): 37.8%\n",
            "Sample: ('COL name VAL WORLD FUEL SERVICES COL name_clean VAL WORLD FUEL SERVICES COL sources VAL [\"fd\"] COL count VAL 1520 COL lobby_count VAL 0 COL amount VAL 41556465.18 COL amount_bc VAL null COL example VAL WORLD FUEL SERVICES', 'COL name VAL SERVICE FUEL SERVICE CANADA COL name_clean VAL SERVICE FUEL SERVICE CANADA COL sources VAL [\"fd\"] COL count VAL 1 COL lobby_count VAL 0 COL amount VAL 0 COL amount_bc VAL null COL example VAL service fuel service canada') -> 1\n"
          ]
        }
      ],
      "source": [
        "class DittoDataset(Dataset):\n",
        "    \"\"\"Dataset for Ditto ER: pairs of serialized records + binary label.\"\"\"\n",
        "\n",
        "    def __init__(self, path, tokenizer, max_len=256, size=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.pairs = []\n",
        "        self.labels = []\n",
        "\n",
        "        lines = open(path).readlines() if isinstance(path, str) else path\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split(\"\\t\")\n",
        "            if len(parts) != 3:\n",
        "                continue\n",
        "            s1, s2, label = parts[0], parts[1], int(parts[2])\n",
        "            self.pairs.append((s1, s2))\n",
        "            self.labels.append(label)\n",
        "\n",
        "        if size:\n",
        "            self.pairs = self.pairs[:size]\n",
        "            self.labels = self.labels[:size]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        left, right = self.pairs[idx]\n",
        "        # Compatible with old (encode_plus) and new (callable) transformers\n",
        "        tokenizer_fn = getattr(self.tokenizer, \"encode_plus\", self.tokenizer)\n",
        "        enc = tokenizer_fn(\n",
        "            left,\n",
        "            right,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        return {\n",
        "            \"input_ids\": torch.stack([b[\"input_ids\"] for b in batch]),\n",
        "            \"attention_mask\": torch.stack([b[\"attention_mask\"] for b in batch]),\n",
        "            \"labels\": torch.stack([b[\"labels\"] for b in batch]),\n",
        "        }\n",
        "\n",
        "# Model name (same as FAIR-DA4ER / megagonlabs ditto)\n",
        "LM_NAME = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(LM_NAME)\n",
        "\n",
        "MAX_LEN = 256\n",
        "\n",
        "# CPU-friendly: randomly subsample train/valid/test to fixed sizes\n",
        "N_TRAIN_IJF = 2000\n",
        "N_VALID_IJF = 250\n",
        "N_TEST_IJF = 250\n",
        "SEED_IJF = 42\n",
        "\n",
        "random.seed(SEED_IJF)\n",
        "np.random.seed(SEED_IJF)\n",
        "\n",
        "def load_and_sample(path, n_target):\n",
        "    \"\"\"Load file and randomly sample n_target lines (or all if file has fewer).\"\"\"\n",
        "    with open(path) as f:\n",
        "        lines = [line.strip() for line in f if line.strip()]\n",
        "    if len(lines) <= n_target:\n",
        "        return lines\n",
        "    idx = np.random.choice(len(lines), size=n_target, replace=False)\n",
        "    return [lines[i] for i in sorted(idx)]\n",
        "\n",
        "train_lines = load_and_sample(trainset_path, N_TRAIN_IJF)\n",
        "valid_lines = load_and_sample(validset_path, N_VALID_IJF)\n",
        "test_lines = load_and_sample(testset_path, N_TEST_IJF)\n",
        "\n",
        "train_ds = DittoDataset(train_lines, tokenizer, max_len=MAX_LEN)\n",
        "valid_ds = DittoDataset(valid_lines, tokenizer, max_len=MAX_LEN)\n",
        "test_ds = DittoDataset(test_lines, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "print(f\"Train: {len(train_ds)}, Valid: {len(valid_ds)}, Test: {len(test_ds)} (random subsample)\")\n",
        "print(f\"Match rate (train): {100 * np.mean(train_ds.labels):.1f}%\")\n",
        "print(\"Sample:\", train_ds.pairs[0], \"->\", train_ds.labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Ditto model (LM + linear head for binary classification)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DittoModel(nn.Module):\n",
        "    \"\"\"Ditto: pre-trained LM with a linear layer for binary match classification.\"\"\"\n",
        "\n",
        "    def __init__(self, lm_name=\"distilbert-base-uncased\", num_labels=2):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(lm_name)\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # [CLS] representation\n",
        "        pooled = outputs.last_hidden_state[:, 0, :]\n",
        "        logits = self.classifier(pooled)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 702.36it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
            "\u001b[1mDistilBertModel LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
            "vocab_transform.bias    | UNEXPECTED |  | \n",
            "vocab_layer_norm.weight | UNEXPECTED |  | \n",
            "vocab_transform.weight  | UNEXPECTED |  | \n",
            "vocab_projector.bias    | UNEXPECTED |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "LR = 3e-5\n",
        "N_EPOCHS = 5\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=DittoDataset.collate_fn,\n",
        ")\n",
        "valid_loader = DataLoader(\n",
        "    valid_ds,\n",
        "    batch_size=BATCH_SIZE * 2,\n",
        "    shuffle=False,\n",
        "    collate_fn=DittoDataset.collate_fn,\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=BATCH_SIZE * 2,\n",
        "    shuffle=False,\n",
        "    collate_fn=DittoDataset.collate_fn,\n",
        ")\n",
        "\n",
        "model = DittoModel(lm_name=LM_NAME).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        "num_training_steps = len(train_loader) * N_EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, loader, threshold=None):\n",
        "    \"\"\"Evaluate model. If threshold is None, find optimal F1 threshold; else use it.\"\"\"\n",
        "    model.eval()\n",
        "    all_labels, all_probs = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"]\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            probs = logits.softmax(dim=1)[:, 1].cpu().numpy()\n",
        "            all_labels.extend(labels.numpy())\n",
        "            all_probs.extend(probs)\n",
        "    if threshold is not None:\n",
        "        preds = [1 if p > threshold else 0 for p in all_probs]\n",
        "        f1 = f1_score(all_labels, preds)\n",
        "        return f1, threshold, preds, all_labels\n",
        "    best_f1, best_th = 0.0, 0.5\n",
        "    for th in np.arange(0.0, 1.0, 0.05):\n",
        "        p = [1 if x > th else 0 for x in all_probs]\n",
        "        f1 = f1_score(all_labels, p)\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_th = f1, th\n",
        "    pred_best = [1 if x > best_th else 0 for x in all_probs]\n",
        "    return best_f1, best_th, pred_best, all_labels\n",
        "\n",
        "def train_epoch(model, loader, optimizer, scheduler, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | loss=0.5062 | dev_f1=0.9043 | test_f1=0.9297 | best_test_f1=0.9297\n",
            "Epoch 2 | loss=0.1321 | dev_f1=0.9462 | test_f1=0.9670 | best_test_f1=0.9670\n",
            "Epoch 3 | loss=0.0673 | dev_f1=0.9348 | test_f1=0.9724 | best_test_f1=0.9670\n",
            "Epoch 4 | loss=0.0290 | dev_f1=0.9362 | test_f1=0.9462 | best_test_f1=0.9670\n",
            "Epoch 5 | loss=0.0169 | dev_f1=0.9355 | test_f1=0.9565 | best_test_f1=0.9670\n"
          ]
        }
      ],
      "source": [
        "best_dev_f1 = 0.0\n",
        "best_test_f1 = 0.0\n",
        "\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    loss = train_epoch(model, train_loader, optimizer, scheduler, criterion)\n",
        "    dev_f1, dev_th, _, _ = evaluate(model, valid_loader)\n",
        "    test_f1, _, _, _ = evaluate(model, test_loader, threshold=dev_th)\n",
        "    if dev_f1 > best_dev_f1:\n",
        "        best_dev_f1 = dev_f1\n",
        "        best_test_f1 = test_f1\n",
        "\n",
        "    print(f\"Epoch {epoch} | loss={loss:.4f} | dev_f1={dev_f1:.4f} | test_f1={test_f1:.4f} | best_test_f1={best_test_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Final test report (using validation threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final test set performance (threshold from validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    No match       0.98      0.97      0.97       158\n",
            "       Match       0.95      0.97      0.96        91\n",
            "\n",
            "    accuracy                           0.97       249\n",
            "   macro avg       0.96      0.97      0.97       249\n",
            "weighted avg       0.97      0.97      0.97       249\n",
            "\n",
            "F1: 0.9565, Precision: 0.9462, Recall: 0.9670\n"
          ]
        }
      ],
      "source": [
        "dev_f1, dev_th, _, _ = evaluate(model, valid_loader)\n",
        "test_f1, _, test_preds, test_labels = evaluate(model, test_loader, threshold=dev_th)\n",
        "\n",
        "print(\"Final test set performance (threshold from validation):\")\n",
        "print(classification_report(test_labels, test_preds, target_names=[\"No match\", \"Match\"]))\n",
        "print(f\"F1: {f1_score(test_labels, test_preds):.4f}, Precision: {precision_score(test_labels, test_preds):.4f}, Recall: {recall_score(test_labels, test_preds):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'test_labels' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Convert labels/predictions to numpy arrays\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m y_true = np.array(\u001b[43mtest_labels\u001b[49m)\n\u001b[32m      6\u001b[39m y_pred = np.array(test_preds)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Indices for each case\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'test_labels' is not defined"
          ]
        }
      ],
      "source": [
        "# Inspect FP, FN, TP, TN examples on the IJF sentence-bert subset\n",
        "import numpy as np\n",
        "\n",
        "# Convert labels/predictions to numpy arrays\n",
        "y_true = np.array(test_labels)\n",
        "y_pred = np.array(test_preds)\n",
        "\n",
        "# Indices for each case\n",
        "tp_idx = np.where((y_true == 1) & (y_pred == 1))[0]\n",
        "fp_idx = np.where((y_true == 0) & (y_pred == 1))[0]\n",
        "tn_idx = np.where((y_true == 0) & (y_pred == 0))[0]\n",
        "fn_idx = np.where((y_true == 1) & (y_pred == 0))[0]\n",
        "\n",
        "print(f\"TP: {len(tp_idx)}, FP: {len(fp_idx)}, TN: {len(tn_idx)}, FN: {len(fn_idx)}\")\n",
        "\n",
        "\n",
        "def show_examples(ds, idxs, kind, k=5):\n",
        "    \"\"\"Pretty-print up to k examples for a given index set.\"\"\"\n",
        "    print(f\"\\n=== {kind} examples (showing up to {k}) ===\")\n",
        "    for idx in idxs[:k]:\n",
        "        rec1, rec2 = ds.pairs[idx]\n",
        "        label = ds.labels[idx]\n",
        "        print(f\"\\nIndex: {idx}\")\n",
        "        print(f\"True label: {label}\")\n",
        "        print(f\"Record 1: {rec1}\")\n",
        "        print(f\"Record 2: {rec2}\")\n",
        "\n",
        "# Show a few examples of each type from the test set\n",
        "# show_examples(test_ds, tp_idx, \"True Positives\")\n",
        "show_examples(test_ds, fp_idx, \"False Positives\")\n",
        "# show_examples(test_ds, tn_idx, \"True Negatives\")\n",
        "# show_examples(test_ds, fn_idx, \"False Negatives\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Predict on new pairs (optional)\n",
        "\n",
        "Serialize two records in Ditto format (`COL attr VAL value ...`) and get match probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Match probability: 0.0973 -> No match\n"
          ]
        }
      ],
      "source": [
        "def predict_pair(model, tokenizer, record1, record2, max_len=256):\n",
        "    \"\"\"Predict match probability for a pair of Ditto-serialized records.\"\"\"\n",
        "    enc = tokenizer(\n",
        "        record1,\n",
        "        record2,\n",
        "        max_length=max_len,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device))\n",
        "        prob_match = logits.softmax(dim=1)[0, 1].item()\n",
        "    return prob_match\n",
        "\n",
        "# Example\n",
        "r1 = \"COL title VAL samsung galaxy COL brand VAL samsung\"\n",
        "r2 = \"COL title VAL samsung galaxy s21 COL brand VAL samsung\"\n",
        "prob = predict_pair(model, tokenizer, r1, r2)\n",
        "print(f\"Match probability: {prob:.4f} -> {'Match' if prob >= 0.5 else 'No match'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Part B: Ditto with Data Augmentation (DA_iTunes-Amazon)\n",
        "\n",
        "This section trains Ditto on **DA_iTunes-Amazon** using **in-training data augmentation (MixDA)** from the repo: the train set uses the `ditto_light` dataset with `da='del'` (span deletion), and the model mixes original and augmented representations during training. Valid/test use no augmentation. Results are reported as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Task (with DA): DA_iTunes-Amazon\n",
            "Train: data/fair/iTunes-Amazon/train.txt, Valid: data/fair/iTunes-Amazon/valid.txt, Test: data/fair/iTunes-Amazon/test.txt\n"
          ]
        }
      ],
      "source": [
        "# Ensure we're in the repo's ditto directory (paths are relative to it)\n",
        "import os\n",
        "try:\n",
        "    _ditto_dir = os.path.join(REPO_DIR, \"ditto\")\n",
        "except NameError:\n",
        "    _ditto_dir = os.path.join(\"FAIR-DA4ER\", \"ditto\")\n",
        "if os.path.isdir(_ditto_dir) and not os.path.isfile(\"configs.json\"):\n",
        "    os.chdir(_ditto_dir)\n",
        "# Load config for DA_iTunes-Amazon (configs[1])\n",
        "with open(\"configs.json\") as f:\n",
        "    configs_da = json.load(f)\n",
        "TASK_NAME_DA = configs_da[1][\"name\"]  # DA_iTunes-Amazon\n",
        "config_da = {c[\"name\"]: c for c in configs_da}[TASK_NAME_DA]\n",
        "trainset_path_da = config_da[\"trainset\"]\n",
        "validset_path_da = config_da[\"validset\"]\n",
        "testset_path_da = config_da[\"testset\"]\n",
        "print(f\"Task (with DA): {TASK_NAME_DA}\")\n",
        "print(f\"Train: {trainset_path_da}, Valid: {validset_path_da}, Test: {testset_path_da}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 321, Valid: 109, Test: 109 (with DA op=del on train)\n"
          ]
        }
      ],
      "source": [
        "# Use ditto_light dataset with in-training augmentation (da='del') for train; no DA for valid/test\n",
        "import sys\n",
        "_ditto_abs = os.path.abspath(_ditto_dir)\n",
        "if _ditto_abs not in sys.path:\n",
        "    sys.path.insert(0, _ditto_abs)\n",
        "from ditto_light.dataset import DittoDataset as DittoDatasetDA\n",
        "from ditto_light.ditto import DittoModel as DittoModelDA\n",
        "\n",
        "MAX_LEN_DA = 256\n",
        "DA_OP = \"del\"  # span deletion (or 'all' for RandAugment)\n",
        "train_ds_da = DittoDatasetDA(trainset_path_da, lm=\"distilbert\", max_len=MAX_LEN_DA, da=DA_OP)\n",
        "valid_ds_da = DittoDatasetDA(validset_path_da, lm=\"distilbert\", max_len=MAX_LEN_DA, da=None)\n",
        "test_ds_da = DittoDatasetDA(testset_path_da, lm=\"distilbert\", max_len=MAX_LEN_DA, da=None)\n",
        "\n",
        "BATCH_SIZE_DA = 8\n",
        "train_loader_da = DataLoader(train_ds_da, batch_size=BATCH_SIZE_DA, shuffle=True, collate_fn=DittoDatasetDA.pad)\n",
        "valid_loader_da = DataLoader(valid_ds_da, batch_size=BATCH_SIZE_DA * 2, shuffle=False, collate_fn=DittoDatasetDA.pad)\n",
        "test_loader_da = DataLoader(test_ds_da, batch_size=BATCH_SIZE_DA * 2, shuffle=False, collate_fn=DittoDatasetDA.pad)\n",
        "print(f\"Train: {len(train_ds_da)}, Valid: {len(valid_ds_da)}, Test: {len(test_ds_da)} (with DA op={DA_OP} on train)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model with MixDA (mixes original + augmented [CLS] during training)\n",
        "set_seed(42)\n",
        "model_da = DittoModelDA(device=device, lm=\"distilbert\", alpha_aug=0.8).to(device)\n",
        "optimizer_da = AdamW(model_da.parameters(), lr=LR)\n",
        "num_steps_da = len(train_loader_da) * N_EPOCHS\n",
        "scheduler_da = get_linear_schedule_with_warmup(optimizer_da, num_warmup_steps=0, num_training_steps=num_steps_da)\n",
        "criterion_da = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch_da(model, loader, optimizer, scheduler, criterion):\n",
        "    \"\"\"Train one epoch; loader returns (x1, x2, y) when DA is used, (x, y) otherwise.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in loader:\n",
        "        optimizer.zero_grad()\n",
        "        if len(batch) == 3:\n",
        "            x1, x2, y = batch\n",
        "            logits = model(x1, x2)\n",
        "        else:\n",
        "            x, y = batch\n",
        "            logits = model(x)\n",
        "        loss = criterion(logits, y.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate_da(model, loader, threshold=None):\n",
        "    \"\"\"Evaluate; valid/test loaders return (x, y) only.\"\"\"\n",
        "    model.eval()\n",
        "    all_labels, all_probs = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            if len(batch) == 3:\n",
        "                x, _, y = batch\n",
        "            else:\n",
        "                x, y = batch\n",
        "            logits = model(x)\n",
        "            probs = logits.softmax(dim=1)[:, 1].cpu().numpy()\n",
        "            all_labels.extend(y.numpy())\n",
        "            all_probs.extend(probs)\n",
        "    if threshold is not None:\n",
        "        preds = [1 if p > threshold else 0 for p in all_probs]\n",
        "        return f1_score(all_labels, preds), threshold, preds, all_labels\n",
        "    best_f1, best_th = 0.0, 0.5\n",
        "    for th in np.arange(0.0, 1.0, 0.05):\n",
        "        p = [1 if x > th else 0 for x in all_probs]\n",
        "        f1 = f1_score(all_labels, p)\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_th = f1, th\n",
        "    pred_best = [1 if x > best_th else 0 for x in all_probs]\n",
        "    return best_f1, best_th, pred_best, all_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## IJF Blocking with Sentence-BERT (top-20 only)\n",
        "\n",
        "Load `pro_supplier_with_clean_and_canonical_trimmed.csv`, apply sentence-BERT blocking (top-20 per record), then create train/test/valid splits and train Ditto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1388738 records from CSV\n",
            "Sample record: COL clean_supplier_name VAL SIMZER DESIGN COL address VAL  COL city VAL  COL prov VAL  COL postal VAL  COL country VAL CA...\n",
            "Canonical ints range: 1 to 157859\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load CSV and serialize records to Ditto format\n",
        "import csv\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# Path to the CSV\n",
        "CSV_PATH_BLOCKING = \"data/ijf/blocking_sentence_bert/pro_supplier_with_clean_and_canonical_trimmed.csv\"\n",
        "\n",
        "# Ensure we're in ditto directory - try multiple paths\n",
        "if not os.path.isfile(CSV_PATH_BLOCKING):\n",
        "    CSV_PATH_BLOCKING = os.path.join(DITTO_DIR, CSV_PATH_BLOCKING)\n",
        "if not os.path.isfile(CSV_PATH_BLOCKING):\n",
        "    # Try relative to current working directory\n",
        "    CSV_PATH_BLOCKING = os.path.join(\"FAIR-DA4ER\", \"ditto\", CSV_PATH_BLOCKING)\n",
        "\n",
        "# Load CSV and serialize to Ditto format\n",
        "records_blocking = []\n",
        "canonical_ints = []\n",
        "with open(CSV_PATH_BLOCKING, encoding=\"utf-8\", newline=\"\") as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    columns = [c for c in reader.fieldnames if c != \"canonical_int\"]  # exclude canonical_int from record text\n",
        "    for i, row in enumerate(reader):\n",
        "        rec_str = \" \".join([f\"COL {col} VAL {row.get(col, '').strip()}\" for col in columns])\n",
        "        records_blocking.append((str(i), rec_str))\n",
        "        canonical_ints.append(int(row.get(\"canonical_int\", 0)))\n",
        "\n",
        "print(f\"Loaded {len(records_blocking)} records from CSV\")\n",
        "print(f\"Sample record: {records_blocking[0][1][:200]}...\")\n",
        "print(f\"Canonical ints range: {min(canonical_ints)} to {max(canonical_ints)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device for encoding and top-k: cuda\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 576.00it/s, Materializing param=pooler.dense.weight]                             \n",
            "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total records: 1,388,738\n",
            "Encoding all records with sentence-BERT (batched)...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 2713/2713 [01:31<00:00, 29.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Freed encoder model; GPU memory available for blocking.\n",
            "\n",
            "Sampling 100 rows (out of 1,388,738) WITHOUT replacement\n",
            "Each sampled row will generate top-1000 similar pairs\n",
            "Expected total: ~100,000 pairs (~100,000 target)\n",
            "Target: <30 min training time on RTX 5090\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-1000 blocking: 100%|██████████| 1/1 [00:00<00:00, 73.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "After sentence-BERT blocking: 100,000 candidate pairs\n",
            "Expected file size: ~0.1 GB (based on ~1.3 KB per pair)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Random row sampling + sentence-BERT blocking (top-500 per sampled row)\n",
        "# Strategy: Randomly sample rows WITHOUT replacement, find top-500 similar pairs per row\n",
        "# Target: ~1.8M pairs total for <30 min training on RTX 5090, with 15-20% match ratio\n",
        "# Increasing TOP_K to 500 helps reduce match percentage (more diverse pairs)\n",
        "import numpy as np\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except ImportError:\n",
        "    import subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sentence-transformers\"])\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "\n",
        "BERT_BATCH_SIZE = 512\n",
        "TOP_K = 1000  # top-500 most similar pairs per sampled row (increased to reduce match %)\n",
        "# Target total pairs: ~1.8M for <30 min training on RTX 5090 GPU\n",
        "# With top-500 per row: sample ~3,600 rows (fewer rows needed since more pairs per row)\n",
        "TARGET_TOTAL_PAIRS = 100_000  # Adjust this to control training time\n",
        "N_SAMPLE_ROWS = TARGET_TOTAL_PAIRS // TOP_K  # ~3,600 rows\n",
        "\n",
        "_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device for encoding and top-k: {_device}\")\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=_device)\n",
        "lines = [rec for _, rec in records_blocking]\n",
        "print(f\"Total records: {len(lines):,}\")\n",
        "\n",
        "# Encode ALL records (needed for similarity search)\n",
        "print(\"Encoding all records with sentence-BERT (batched)...\")\n",
        "vecs = model.encode(\n",
        "    lines,\n",
        "    batch_size=BERT_BATCH_SIZE,\n",
        "    show_progress_bar=True,\n",
        "    normalize_embeddings=True,\n",
        "    convert_to_numpy=True,\n",
        ")\n",
        "V_np = np.float32(vecs)\n",
        "n_total = len(V_np)\n",
        "del vecs\n",
        "\n",
        "# Free GPU memory: drop the encoder model\n",
        "if _device == \"cuda\":\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    print(f\"Freed encoder model; GPU memory available for blocking.\")\n",
        "\n",
        "# Move embeddings to GPU for fast matmul + topk\n",
        "use_gpu = _device == \"cuda\"\n",
        "if use_gpu:\n",
        "    V = torch.from_numpy(V_np).to(_device)\n",
        "else:\n",
        "    V = V_np\n",
        "\n",
        "# Randomly sample rows to process (WITHOUT replacement - each row selected only once)\n",
        "np.random.seed(42)\n",
        "n_sample = min(N_SAMPLE_ROWS, n_total)\n",
        "sampled_indices = np.random.choice(n_total, size=n_sample, replace=False)  # replace=False ensures no duplicates\n",
        "sampled_indices = np.sort(sampled_indices)  # Sort for better cache locality\n",
        "print(f\"\\nSampling {n_sample:,} rows (out of {n_total:,}) WITHOUT replacement\")\n",
        "print(f\"Each sampled row will generate top-{TOP_K} similar pairs\")\n",
        "print(f\"Expected total: ~{n_sample * TOP_K:,} pairs (~{TARGET_TOTAL_PAIRS:,} target)\")\n",
        "print(f\"Target: <30 min training time on RTX 5090\")\n",
        "\n",
        "# Process sampled rows: for each sampled row, find top-500 similar pairs\n",
        "pairs_bert = []\n",
        "BATCH_SIZE_SAMPLE = 2000  # Process sampled rows in batches to manage GPU memory\n",
        "\n",
        "for batch_start in tqdm(range(0, len(sampled_indices), BATCH_SIZE_SAMPLE), desc=f\"Top-{TOP_K} blocking\"):\n",
        "    batch_end = min(batch_start + BATCH_SIZE_SAMPLE, len(sampled_indices))\n",
        "    batch_indices = sampled_indices[batch_start:batch_end]\n",
        "    batch_size = len(batch_indices)\n",
        "    \n",
        "    if use_gpu:\n",
        "        # Get embeddings for this batch of sampled rows\n",
        "        batch_vecs = V[batch_indices]  # (batch_size, dim)\n",
        "        # Compute similarity: batch_vecs @ V.T -> (batch_size, n_total)\n",
        "        sim = batch_vecs @ V.T\n",
        "        \n",
        "        # Exclude self: for each sampled row j, set sim[j_local, batch_indices[j_local]] = -inf\n",
        "        j_local = torch.arange(batch_size, device=_device)\n",
        "        self_indices = torch.tensor(batch_indices, device=_device)\n",
        "        sim[j_local, self_indices] = -1e9\n",
        "        \n",
        "        # Get top-500 for each sampled row\n",
        "        _, topk_idx = sim.topk(min(TOP_K, sim.shape[1]), dim=1)  # (batch_size, TOP_K)\n",
        "        \n",
        "        # Build pairs: (similar_row_idx, sampled_row_idx)\n",
        "        sampled_row_indices = torch.tensor(batch_indices, device=_device).repeat_interleave(TOP_K)\n",
        "        pairs_batch = torch.stack([topk_idx.flatten(), sampled_row_indices], dim=1)\n",
        "        pairs_bert.extend(pairs_batch.cpu().tolist())\n",
        "    else:\n",
        "        # CPU fallback\n",
        "        batch_vecs = V[batch_indices]\n",
        "        sim = batch_vecs @ V.T\n",
        "        for j_local, j in enumerate(batch_indices):\n",
        "            col = np.asarray(sim[j_local, :]).ravel()\n",
        "            col[j] = -np.inf\n",
        "            kth = min(TOP_K, col.size) - 1\n",
        "            if kth < 0:\n",
        "                continue\n",
        "            top500 = np.argpartition(-col, kth)[:TOP_K]\n",
        "            for i in top500:\n",
        "                pairs_bert.append((int(i), j))\n",
        "\n",
        "print(f\"\\nAfter sentence-BERT blocking: {len(pairs_bert):,} candidate pairs\")\n",
        "print(f\"Expected file size: ~{len(pairs_bert) * 1353 / (1024**3):.1f} GB (based on ~1.3 KB per pair)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Labeled pairs: 100000\n",
            "Matches (label=1): 34222 (34.2%)\n",
            "Non-matches (label=0): 65778 (65.8%)\n",
            "\n",
            "Split sizes:\n",
            "  Train: 79999\n",
            "  Valid: 9999\n",
            "  Test: 10002\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Label pairs (same canonical_int = 1) and split into train/test/valid\n",
        "labeled_pairs = []\n",
        "for i, j in pairs_bert:\n",
        "    label = 1 if canonical_ints[i] == canonical_ints[j] and canonical_ints[i] != 0 else 0\n",
        "    labeled_pairs.append((i, j, label))\n",
        "\n",
        "labeled_pairs = np.array(labeled_pairs, dtype=object)\n",
        "labels = np.array([x[2] for x in labeled_pairs])\n",
        "n_pos = int(np.sum(labels == 1))\n",
        "n_neg = int(np.sum(labels == 0))\n",
        "\n",
        "print(f\"Labeled pairs: {len(labeled_pairs)}\")\n",
        "print(f\"Matches (label=1): {n_pos} ({100*n_pos/len(labeled_pairs):.1f}%)\")\n",
        "print(f\"Non-matches (label=0): {n_neg} ({100*n_neg/len(labeled_pairs):.1f}%)\")\n",
        "\n",
        "# Stratified split: 80% train, 10% valid, 10% test\n",
        "SEED_BLOCKING = 42\n",
        "np.random.seed(SEED_BLOCKING)\n",
        "pos_idx = np.where(labels == 1)[0]\n",
        "neg_idx = np.where(labels == 0)[0]\n",
        "np.random.shuffle(pos_idx)\n",
        "np.random.shuffle(neg_idx)\n",
        "\n",
        "def split_indices(idxs, train_ratio=0.8, valid_ratio=0.1):\n",
        "    n = len(idxs)\n",
        "    if n == 0:\n",
        "        return [], [], []\n",
        "    t = max(1, int(n * train_ratio))\n",
        "    v = max(0, int(n * valid_ratio))\n",
        "    te = n - t - v\n",
        "    return idxs[:t], idxs[t:t+v], idxs[t+v:]\n",
        "\n",
        "pos_t, pos_v, pos_te = split_indices(pos_idx)\n",
        "neg_t, neg_v, neg_te = split_indices(neg_idx)\n",
        "\n",
        "train_idx = np.concatenate([pos_t, neg_t])\n",
        "valid_idx = np.concatenate([pos_v, neg_v])\n",
        "test_idx = np.concatenate([pos_te, neg_te])\n",
        "np.random.shuffle(train_idx)\n",
        "np.random.shuffle(valid_idx)\n",
        "np.random.shuffle(test_idx)\n",
        "\n",
        "print(f\"\\nSplit sizes:\")\n",
        "print(f\"  Train: {len(train_idx)}\")\n",
        "print(f\"  Valid: {len(valid_idx)}\")\n",
        "print(f\"  Test: {len(test_idx)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote train.txt, valid.txt, test.txt to /workspace/Ditto/FAIR-DA4ER/FAIR-DA4ER/ditto/data/ijf/blocking_sentence_bert\n",
            "\n",
            "=== Dataset Statistics ===\n",
            "Train: 79999 pairs | Matches: 27377 (34.2%) | Non-matches: 52622 (65.8%)\n",
            "Valid: 9999 pairs | Matches: 3422 (34.2%) | Non-matches: 6577 (65.8%)\n",
            "Test:  10002 pairs | Matches: 3423 (34.2%) | Non-matches: 6579 (65.8%)\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Write train.txt, test.txt, valid.txt\n",
        "OUTPUT_DIR_BLOCKING = os.path.join(DITTO_DIR, \"data/ijf/blocking_sentence_bert\")\n",
        "os.makedirs(OUTPUT_DIR_BLOCKING, exist_ok=True)\n",
        "\n",
        "def write_split(split_name, indices):\n",
        "    path = os.path.join(OUTPUT_DIR_BLOCKING, f\"{split_name}.txt\")\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for idx in indices:\n",
        "            i, j, label = labeled_pairs[idx]\n",
        "            rec1 = records_blocking[i][1]\n",
        "            rec2 = records_blocking[j][1]\n",
        "            f.write(f\"{rec1}\\t{rec2}\\t{label}\\n\")\n",
        "    return path\n",
        "\n",
        "train_path = write_split(\"train\", train_idx)\n",
        "valid_path = write_split(\"valid\", valid_idx)\n",
        "test_path = write_split(\"test\", test_idx)\n",
        "\n",
        "print(f\"Wrote train.txt, valid.txt, test.txt to {OUTPUT_DIR_BLOCKING}\")\n",
        "\n",
        "# Report statistics\n",
        "train_labels = np.array([labeled_pairs[i][2] for i in train_idx])\n",
        "valid_labels = np.array([labeled_pairs[i][2] for i in valid_idx])\n",
        "test_labels_blocking = np.array([labeled_pairs[i][2] for i in test_idx])\n",
        "\n",
        "print(f\"\\n=== Dataset Statistics ===\")\n",
        "print(f\"Train: {len(train_idx)} pairs | Matches: {int(train_labels.sum())} ({100*train_labels.mean():.1f}%) | Non-matches: {len(train_idx)-int(train_labels.sum())} ({100*(1-train_labels.mean()):.1f}%)\")\n",
        "print(f\"Valid: {len(valid_idx)} pairs | Matches: {int(valid_labels.sum())} ({100*valid_labels.mean():.1f}%) | Non-matches: {len(valid_idx)-int(valid_labels.sum())} ({100*(1-valid_labels.mean()):.1f}%)\")\n",
        "print(f\"Test:  {len(test_idx)} pairs | Matches: {int(test_labels_blocking.sum())} ({100*test_labels_blocking.mean():.1f}%) | Non-matches: {len(test_idx)-int(test_labels_blocking.sum())} ({100*(1-test_labels_blocking.mean()):.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 6: Loading datasets and creating model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 498.49it/s, Materializing param=transformer.layer.5.sa_layer_norm.weight]   \n",
            "\u001b[1mDistilBertModel LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "vocab_layer_norm.bias   | UNEXPECTED |  | \n",
            "vocab_transform.bias    | UNEXPECTED |  | \n",
            "vocab_layer_norm.weight | UNEXPECTED |  | \n",
            "vocab_transform.weight  | UNEXPECTED |  | \n",
            "vocab_projector.bias    | UNEXPECTED |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset: 79,999 pairs\n",
            "Valid dataset: 9,999 pairs\n",
            "Test dataset: 10,002 pairs\n",
            "\n",
            "Optimized Training Settings:\n",
            "  Batch size (train): 64 (was 8)\n",
            "  Batch size (eval): 128 (was 16)\n",
            "  Batches per epoch (train): 1,250 (was 9,999)\n",
            "  DataLoader workers: 4 (parallel loading)\n",
            "  Pin memory: True (faster GPU transfer)\n",
            "\n",
            "Estimated Training Time (RTX 5090):\n",
            "  Per epoch: ~3-4 minutes\n",
            "  Total (5 epochs): ~15-20 minutes\n",
            "  Speedup: ~14-28x faster than batch_size=8\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 6: Load datasets and create model\n",
        "print(\"Step 6: Loading datasets and creating model...\")\n",
        "train_ds_blocking = DittoDataset(train_path, tokenizer, max_len=MAX_LEN)\n",
        "valid_ds_blocking = DittoDataset(valid_path, tokenizer, max_len=MAX_LEN)\n",
        "test_ds_blocking = DittoDataset(test_path, tokenizer, max_len=MAX_LEN)\n",
        "\n",
        "# Optimized batch sizes for RTX 5090 (much faster training)\n",
        "BATCH_SIZE_TRAIN = 64   # Increased from 8 → 64 (8x larger batches = 8x fewer iterations)\n",
        "BATCH_SIZE_EVAL = 128   # Larger for validation/test (faster evaluation)\n",
        "\n",
        "# DataLoader optimizations: num_workers for parallel loading, pin_memory for faster GPU transfer\n",
        "train_loader_blocking = DataLoader(\n",
        "    train_ds_blocking, \n",
        "    batch_size=BATCH_SIZE_TRAIN, \n",
        "    shuffle=True, \n",
        "    collate_fn=DittoDataset.collate_fn,\n",
        "    num_workers=4,        # Parallel data loading\n",
        "    pin_memory=True,      # Faster GPU transfer\n",
        "    persistent_workers=True  # Keep workers alive between epochs\n",
        ")\n",
        "valid_loader_blocking = DataLoader(\n",
        "    valid_ds_blocking, \n",
        "    batch_size=BATCH_SIZE_EVAL, \n",
        "    shuffle=False, \n",
        "    collate_fn=DittoDataset.collate_fn,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "test_loader_blocking = DataLoader(\n",
        "    test_ds_blocking, \n",
        "    batch_size=BATCH_SIZE_EVAL, \n",
        "    shuffle=False, \n",
        "    collate_fn=DittoDataset.collate_fn,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Create new model for this run\n",
        "model_blocking = DittoModel(lm_name=LM_NAME).to(device)\n",
        "optimizer_blocking = AdamW(model_blocking.parameters(), lr=LR)\n",
        "num_steps_blocking = len(train_loader_blocking) \n",
        "scheduler_blocking = get_linear_schedule_with_warmup(optimizer_blocking, num_warmup_steps=0, num_training_steps=num_steps_blocking)\n",
        "criterion_blocking = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"Train dataset: {len(train_ds_blocking):,} pairs\")\n",
        "print(f\"Valid dataset: {len(valid_ds_blocking):,} pairs\")\n",
        "print(f\"Test dataset: {len(test_ds_blocking):,} pairs\")\n",
        "print()\n",
        "print(\"Optimized Training Settings:\")\n",
        "print(f\"  Batch size (train): {BATCH_SIZE_TRAIN} (was 8)\")\n",
        "print(f\"  Batch size (eval): {BATCH_SIZE_EVAL} (was 16)\")\n",
        "print(f\"  Batches per epoch (train): {len(train_loader_blocking):,} (was {len(train_ds_blocking)//8:,})\")\n",
        "print(f\"  DataLoader workers: 4 (parallel loading)\")\n",
        "print(f\"  Pin memory: True (faster GPU transfer)\")\n",
        "print()\n",
        "print(\"Estimated Training Time (RTX 5090):\")\n",
        "print(f\"  Per epoch: ~3-4 minutes\")\n",
        "print(f\"  Total ({N_EPOCHS} epochs): ~15-20 minutes\")\n",
        "print(f\"  Speedup: ~14-28x faster than batch_size=8\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | loss=0.0034 | dev_f1=0.9985 | test_f1=0.9988 | best_test_f1=0.9988\n",
            "Epoch 2 | loss=0.0035 | dev_f1=0.9985 | test_f1=0.9988 | best_test_f1=0.9988\n",
            "Epoch 3 | loss=0.0037 | dev_f1=0.9985 | test_f1=0.9988 | best_test_f1=0.9988\n",
            "Epoch 4 | loss=0.0033 | dev_f1=0.9985 | test_f1=0.9988 | best_test_f1=0.9988\n",
            "Epoch 5 | loss=0.0038 | dev_f1=0.9985 | test_f1=0.9988 | best_test_f1=0.9988\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Training loop\n",
        "set_seed(SEED_BLOCKING)\n",
        "best_dev_f1_blocking = 0.0\n",
        "best_test_f1_blocking = 0.0\n",
        "\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    loss = train_epoch(\n",
        "        model_blocking,\n",
        "        train_loader_blocking,\n",
        "        optimizer_blocking,\n",
        "        scheduler_blocking,\n",
        "        criterion_blocking,\n",
        "    )\n",
        "    dev_f1, dev_th, _, _ = evaluate(model_blocking, valid_loader_blocking)\n",
        "    test_f1, _, _, _ = evaluate(model_blocking, test_loader_blocking, threshold=dev_th)\n",
        "\n",
        "    if dev_f1 > best_dev_f1_blocking:\n",
        "        best_dev_f1_blocking = dev_f1\n",
        "        best_test_f1_blocking = test_f1\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch} | loss={loss:.4f} \"\n",
        "        f\"| dev_f1={dev_f1:.4f} | test_f1={test_f1:.4f} \"\n",
        "        f\"| best_test_f1={best_test_f1_blocking:.4f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Final Test Set Performance (IJF Blocking, sentence-BERT top-20) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    No match       1.00      1.00      1.00      6579\n",
            "       Match       1.00      1.00      1.00      3423\n",
            "\n",
            "    accuracy                           1.00     10002\n",
            "   macro avg       1.00      1.00      1.00     10002\n",
            "weighted avg       1.00      1.00      1.00     10002\n",
            "\n",
            "\n",
            "=== Detailed Metrics ===\n",
            "Accuracy:  0.9992\n",
            "Precision: 0.9977\n",
            "Recall:    1.0000\n",
            "F1:        0.9988\n",
            "\n",
            "=== Confusion Matrix ===\n",
            "TP (True Positives):  3423\n",
            "FP (False Positives): 8\n",
            "TN (True Negatives):  6571\n",
            "FN (False Negatives): 0\n",
            "\n",
            "Total test pairs: 10002\n"
          ]
        }
      ],
      "source": [
        "# Step 8: Final evaluation on test set with detailed metrics\n",
        "dev_f1_blocking, dev_th_blocking, _, _ = evaluate(model_blocking, valid_loader_blocking)\n",
        "test_f1_blocking, _, test_preds_blocking, test_labels_blocking_final = evaluate(model_blocking, test_loader_blocking, threshold=dev_th_blocking)\n",
        "\n",
        "# Compute TP, FP, TN, FN\n",
        "y_true = np.array(test_labels_blocking_final)\n",
        "y_pred = np.array(test_preds_blocking)\n",
        "\n",
        "tp = int(np.sum((y_true == 1) & (y_pred == 1)))\n",
        "fp = int(np.sum((y_true == 0) & (y_pred == 1)))\n",
        "tn = int(np.sum((y_true == 0) & (y_pred == 0)))\n",
        "fn = int(np.sum((y_true == 1) & (y_pred == 0)))\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = (tp + tn) / len(y_true) if len(y_true) > 0 else 0.0\n",
        "precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "print(\"=== Final Test Set Performance (IJF Blocking, sentence-BERT top-20) ===\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"No match\", \"Match\"]))\n",
        "print(f\"\\n=== Detailed Metrics ===\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1:        {f1:.4f}\")\n",
        "print(f\"\\n=== Confusion Matrix ===\")\n",
        "print(f\"TP (True Positives):  {tp}\")\n",
        "print(f\"FP (False Positives): {fp}\")\n",
        "print(f\"TN (True Negatives):  {tn}\")\n",
        "print(f\"FN (False Negatives): {fn}\")\n",
        "print(f\"\\nTotal test pairs: {len(y_true)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Example Analysis ===\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'show_examples' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m fn_idx_blocking = np.where((y_true == \u001b[32m1\u001b[39m) & (y_pred == \u001b[32m0\u001b[39m))[\u001b[32m0\u001b[39m]\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Example Analysis ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mshow_examples\u001b[49m(test_ds_blocking, tp_idx_blocking, \u001b[33m\"\u001b[39m\u001b[33mTrue Positives (Blocking)\u001b[39m\u001b[33m\"\u001b[39m, k=\u001b[32m3\u001b[39m)\n\u001b[32m      9\u001b[39m show_examples(test_ds_blocking, fp_idx_blocking, \u001b[33m\"\u001b[39m\u001b[33mFalse Positives (Blocking)\u001b[39m\u001b[33m\"\u001b[39m, k=\u001b[32m3\u001b[39m)\n\u001b[32m     10\u001b[39m show_examples(test_ds_blocking, tn_idx_blocking, \u001b[33m\"\u001b[39m\u001b[33mTrue Negatives (Blocking)\u001b[39m\u001b[33m\"\u001b[39m, k=\u001b[32m3\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'show_examples' is not defined"
          ]
        }
      ],
      "source": [
        "# Step 9: Show examples of TP, FP, TN, FN\n",
        "tp_idx_blocking = np.where((y_true == 1) & (y_pred == 1))[0]\n",
        "fp_idx_blocking = np.where((y_true == 0) & (y_pred == 1))[0]\n",
        "tn_idx_blocking = np.where((y_true == 0) & (y_pred == 0))[0]\n",
        "fn_idx_blocking = np.where((y_true == 1) & (y_pred == 0))[0]\n",
        "\n",
        "print(f\"\\n=== Example Analysis ===\")\n",
        "show_examples(test_ds_blocking, tp_idx_blocking, \"True Positives (Blocking)\", k=3)\n",
        "show_examples(test_ds_blocking, fp_idx_blocking, \"False Positives (Blocking)\", k=3)\n",
        "show_examples(test_ds_blocking, tn_idx_blocking, \"True Negatives (Blocking)\", k=3)\n",
        "show_examples(test_ds_blocking, fn_idx_blocking, \"False Negatives (Blocking)\", k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'test_labels' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Convert labels/predictions to numpy arrays\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m y_true = np.array(\u001b[43mtest_labels\u001b[49m)\n\u001b[32m      6\u001b[39m y_pred = np.array(test_preds)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Indices for each case\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'test_labels' is not defined"
          ]
        }
      ],
      "source": [
        "# Inspect FP, FN, TP, TN examples on the IJF sentence-bert subset\n",
        "import numpy as np\n",
        "\n",
        "# Convert labels/predictions to numpy arrays\n",
        "y_true = np.array(test_labels)\n",
        "y_pred = np.array(test_preds)\n",
        "\n",
        "# Indices for each case\n",
        "tp_idx = np.where((y_true == 1) & (y_pred == 1))[0]\n",
        "fp_idx = np.where((y_true == 0) & (y_pred == 1))[0]\n",
        "tn_idx = np.where((y_true == 0) & (y_pred == 0))[0]\n",
        "fn_idx = np.where((y_true == 1) & (y_pred == 0))[0]\n",
        "\n",
        "print(f\"TP: {len(tp_idx)}, FP: {len(fp_idx)}, TN: {len(tn_idx)}, FN: {len(fn_idx)}\")\n",
        "\n",
        "\n",
        "def show_examples(ds, idxs, kind, k=5):\n",
        "    \"\"\"Pretty-print up to k examples for a given index set.\"\"\"\n",
        "    print(f\"\\n=== {kind} examples (showing up to {k}) ===\")\n",
        "    for idx in idxs[:k]:\n",
        "        rec1, rec2 = ds.pairs[idx]\n",
        "        label = ds.labels[idx]\n",
        "        print(f\"\\nIndex: {idx}\")\n",
        "        print(f\"True label: {label}\")\n",
        "        print(f\"Record 1: {rec1}\")\n",
        "        print(f\"Record 2: {rec2}\")\n",
        "\n",
        "# Show a few examples of each type from the test set\n",
        "show_examples(test_ds, tp_idx, \"True Positives\")\n",
        "show_examples(test_ds, fp_idx, \"False Positives\")\n",
        "show_examples(test_ds, tn_idx, \"True Negatives\")\n",
        "show_examples(test_ds, fn_idx, \"False Negatives\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | loss=0.5796 | dev_f1=0.5806 | test_f1=0.5926 | best_test_f1=0.5926\n",
            "Epoch 2 | loss=0.4715 | dev_f1=0.6316 | test_f1=0.6667 | best_test_f1=0.6667\n",
            "Epoch 3 | loss=0.2959 | dev_f1=0.8475 | test_f1=0.8621 | best_test_f1=0.8621\n",
            "Epoch 4 | loss=0.1591 | dev_f1=0.8966 | test_f1=0.9123 | best_test_f1=0.9123\n",
            "Epoch 5 | loss=0.1178 | dev_f1=0.8889 | test_f1=0.8889 | best_test_f1=0.9123\n",
            "Epoch 6 | loss=0.0505 | dev_f1=0.8889 | test_f1=0.9091 | best_test_f1=0.9123\n",
            "Epoch 7 | loss=0.1673 | dev_f1=0.8814 | test_f1=0.8966 | best_test_f1=0.9123\n",
            "Epoch 8 | loss=0.0305 | dev_f1=0.8772 | test_f1=0.9310 | best_test_f1=0.9123\n",
            "Epoch 9 | loss=0.0241 | dev_f1=0.8772 | test_f1=0.9310 | best_test_f1=0.9123\n",
            "Epoch 10 | loss=0.0134 | dev_f1=0.8772 | test_f1=0.9123 | best_test_f1=0.9123\n"
          ]
        }
      ],
      "source": [
        "# Training loop (Ditto with DA)\n",
        "best_dev_f1_da = 0.0\n",
        "best_test_f1_da = 0.0\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    loss = train_epoch_da(model_da, train_loader_da, optimizer_da, scheduler_da, criterion_da)\n",
        "    dev_f1, dev_th, _, _ = evaluate_da(model_da, valid_loader_da)\n",
        "    test_f1, _, _, _ = evaluate_da(model_da, test_loader_da, threshold=dev_th)\n",
        "    if dev_f1 > best_dev_f1_da:\n",
        "        best_dev_f1_da = dev_f1\n",
        "        best_test_f1_da = test_f1\n",
        "    print(f\"Epoch {epoch} | loss={loss:.4f} | dev_f1={dev_f1:.4f} | test_f1={test_f1:.4f} | best_test_f1={best_test_f1_da:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'evaluate_da' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Final test report (Ditto with DA) — same format as before\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dev_f1_da, dev_th_da, _, _ = \u001b[43mevaluate_da\u001b[49m(model_da, valid_loader_da)\n\u001b[32m      3\u001b[39m test_f1_da, _, test_preds_da, test_labels_da = evaluate_da(model_da, test_loader_da, threshold=dev_th_da)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPart B — \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTASK_NAME_DA\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (with data augmentation):\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'evaluate_da' is not defined"
          ]
        }
      ],
      "source": [
        "# Final test report (Ditto with DA) — same format as before\n",
        "dev_f1_da, dev_th_da, _, _ = evaluate_da(model_da, valid_loader_da)\n",
        "test_f1_da, _, test_preds_da, test_labels_da = evaluate_da(model_da, test_loader_da, threshold=dev_th_da)\n",
        "print(f\"Part B — {TASK_NAME_DA} (with data augmentation):\")\n",
        "print(\"Final test set performance (threshold from validation):\")\n",
        "print(classification_report(test_labels_da, test_preds_da, target_names=[\"No match\", \"Match\"]))\n",
        "print(f\"F1: {f1_score(test_labels_da, test_preds_da):.4f}, Precision: {precision_score(test_labels_da, test_preds_da):.4f}, Recall: {recall_score(test_labels_da, test_preds_da):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
